### Semantic Web: Priesthoods

The term "Knowledge Graph" evolved out of the Semantic Web project {% cite hitzlerReviewSemanticWeb2021 %}, and so we rewind to the start point of our history at the turn of the millennium. It is difficult to reconstruct how radical the notion of a collection of documents organized by arbitrary links between them was at dawn of the internet. At the time, the infrastructures of linking documents looked more like ISBNs, carefully regulated by expert, centralized authorities[^dois]. Being able to *just link to anything* was *terrifying* and *new* (eg. {% cite berners-leeLinksLaw1997 berners-leeLinksLawMyths1997 %}).

The initial design of the web imagined it as a self-organizing process, where people would maintain their own websites and organize a collection of links to other websites[^wikibus]. It became clear relatively quickly that the anarchy of a socially self-organizing internet wasn't going to work as planned, where without a formal system of organization "people were frightened of getting lost in it. You could follow links forever." {% cite berners-leeWhatSemanticWeb1998 %} 

Like the radical nature of linking on the web, it's difficult to remember that the web as surveillance apparatus thinly veiled as the five or so remaining platform-websites was not inevitable. The pre-dotcom bust internet of the 90's and early 2000's was far from the commercialized wasteland we know today. Ed Horowitz, CEO of Viacom explained in 1996: "The Internet has yet to fulfill its promise of commercial success. Why? Because there is no business model" {% cite tarnoffInternetPeopleFight2022 %}. Google's AdWords being a defining moment in the development of surveillance capitalism is a story already told {% cite zuboffAgeSurveillanceCapitalism2019 %}: taking advantage of the need for search generated by the disorganization of the web, AdWords turned personal search data into a profit vector by selling targeted space in the results.

The significance of the relationship between search, the semantic web, and what became knowledge graphs is less widely appreciated. The semantic web was initially an alternative to monolithic search engine platforms - or, more generally, to platforms in general {% cite berners-leeSociallyAwareCloud2009 %}. It imagined the use of triplet links and shared ontologies at a protocol level as a way of organizing the information on the web into a richly explorable space: rather than needing to rely on a search bar, one could traverse a structured graph of information {% cite berners-leeLinkedData2006 berners-leeGoalsHumanDataInterface2010 %} to find what one needed without mediation by a third party.

The Semantic Web project was an attempt to supplement the arbitrary power to express human-readable information in linked documents with computer-readable information. It imagined a linked and overlapping set of schemas ranging from locally expressive vocabularies used among small groups of friends through globally shared, logically consistent ontologies. The semantic web was intended to evolve fluidly, like language, with cultures of meaning meshing and separating at multiple scales {% cite berners-leeScalefreeNatureWeb1998 berners-leeSemanticWeb2001 berners-leeCulturesBoundaries2007 %}:

> Locally defined languages are easy to create, needing local consensus about meaning: only a limited number of people have to share a mental pattern of relationships which define the meaning. However, global languages are so much more effective at communication, reaching the parts that local languages cannot. [...]
>
>  So the idea is that in any one message, some of the terms will be from a global ontology, some from subdomains. The amount of data which can be reused by another agent will depend on how many communities they have in common, how many ontologies they share.
> 
> In other words, one global ontology is not a solution to the problem, and a local subdomain is not a solution either. But if each agent has uses a mix of a few ontologies of different scale, that is forms a global solution to the problem. {% cite berners-leeScalefreeNatureWeb1998 %}

> The Semantic Web, in naming every concept simply by a URI, lets anyone express new concepts that they invent with minimal effort. Its unifying logical language will enable these concepts to be progressively linked into a universal Web. {% cite berners-leeSemanticWeb2001 %}


This free form goal of expression for expression's sake was always in tension with another part of the vision - serving as a backbone for AI "agents" that could compute emergent function from the semantic web. Succinctly: "Human language thrives when using the same term to mean somewhat different things, but automation does not." {% cite berners-leeSemanticWeb2001 %} This tension persists through the broader history of the web, and [we will return to it soon](#the-near-future-of-surveillance-capitalism-knowledge-graphs-get-chatbots). 