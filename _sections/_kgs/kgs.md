### Knowledge Graphs: Panoptica

In 2010 Google acquired Metaweb and its publicly-edited Semantic Web database Freebase, and in 2012 repackaged it and the ideas of Linked Data as what it called a **Knowledge Graph** --- the third era of the Semantic Web {% cite singhalIntroducingKnowledgeGraph2012 iainFreebaseDeadLong2016 %}. Freebase only made up part of it, and the full extent of Google's Knowledge Graph are unknown, but its most visible impact are the factboxes that present structured information about the subjects of searches - like biographical information in a search for a person - or the different widgets for contextual interaction - like being able to make a restaurant reservation from the serach page {% cite noyIndustryscaleKnowledgeGraphs2019 %}. Knowledge Graphs still share the same underlying structure --- triplet graphs with ontologies --- even if they occupy a broader space of implementations and technologies. What differs is the context and intended use: the "worldview" of the knowledge graph.

Beyond the obvious product-level features it supports, Google's acquisition of Freebase and the structure of its Knowledge Graph represent at least two deeper shifts in the trajectory of the Semantic Web and the broader internet: the privatization of technologies with initially liberatory aspirations, and an early template of the sprawling, surveillance-driven information conglomerate we know and love today. 

Like the radical nature of linking on the web, it's difficult to remember that the web as surveillance apparatus thinly veiled as the five or so remaining websites was not inevitable. The pre-dotcom bust internet of the 90's and early 2000's was far from the commercialized wasteland we know today. Ed Horowitz, CEO of Viacom explained in 1996: "The Internet has yet to fulfill its promise of commercial success. Why? Because there is no business model" {% cite tarnoffInternetPeopleFight2022 %}. Google's AdWords being a defining moment in the development of surveillance capitalism is a story already told {% cite zuboffAgeSurveillanceCapitalism2019 %}: taking advantage of the need for search generated by the disorganization of the web, AdWords turned personal search data into a profit vector by selling targeted space in the results.

The significance of the relationship between search, the semantic web, and what became knowledge graphs is less widely appreciated. The semantic web was initially an alternative to monolithic search engine platforms - or, more generally, to platforms in general {% cite berners-leeSociallyAwareCloud2009 %}. It imagined the use of triplet links and shared ontologies at a protocol level as a way of organizing the information on the web into a richly explorable space: rather than needing to rely on a search bar, one could traverse a structured graph of information {% cite berners-leeLinkedData2006 berners-leeGoalsHumanDataInterface2010 %} to find what one needed without mediation by a third party.

Instead, the form of of the semantic web that emerged as "Knowledge Graphs" flipped the vision of a free and evolving internet on its head. The mutation from "Linked Open Data" {% cite berners-leeLinkedData2006 %} to "Knowledge Graphs" is a shift in meaning from a public and densely linked web of information from many sources to a proprietary information store used to power derivative platforms and services. The shift isn't quite so simple as a "closure" of a formerly open resource --- we'll return to the complex role of openness in a moment. It is closer to an *en*closure, a *domestication* of the dream of the Semantic Web. A dream of a mutating, pluralistic space of communication, where we were able to own and change and create the information that structures our digital lives was reduced to a ring of platforms that give us precisely as much agency as is needed to keep us content in our captivity. Links that had all the expressive power of utterances, questions, hints, slander, and lies were reduced to mere facts. We were recast from our role as *people* creating a digital world to *consumers* of subscriptions and services. The artifacts that we create for and with and between each other as the substance of our lives online were yoked to the acquisitive gaze of the knowledge graph as *content* to be mined. We vulgar commoners, we data subjects, are not allowed to touch the graph --- even if it is built from our disembodied bits.

The same technologies, with minor variation, that were intended to keep the internet free became emblematic of and coproductive with the surveillance/platform model that has enclosed it. Beyond Google, knowledge graphs are an elemental part of the contempory information economy. Banks, militaries, governments, life science corporations, journalists, everyone is using knowledge graphs {% cite neo4jNeo4jCustomers enterpriseknowledgegraphfoundationKnowledgeGraphIndustry2022 %}. Their ubiquity is not an accident, one of many possible data systems that could have fit the bill, but reflects and reinforces basic patterns of the information economy and the corporations within it. 

What makes knowledge graphs so special? It turns out that semantic web technologies, designed to accomodate the infinitely heterogeneous, multiscale nature of free and unmediated social structuring of information are also quite useful for the indefinitely expanding dragnet of data collection that defines the operation of contemporary capitalism:

> "If one takes a look at the top Fortune 500 companies, it is surprising how many of them are really in the information business. I don’t just mean the technology and telecommunication companies like Apple or Google or Verizon or Cisco or the drug companies like Pfizer. One could also think of the big banks as a subset of the vectoralist class rather than as “finance capital.” They too are in the information asymmetry business. And as we learned in the 2008 crash, even the car companies are in the information business—they made more money from car loans than cars. The military—industrial sector is also in the information business. The companies that appear to sell actual things, like Nike, are really in the brand business. Walmart and Amazon compete with different models of the information logistics business. Even the oil companies are in part at least in the information-about-the-geology-of-possible-oil-deposits business. Perhaps the vectoralist class is no longer emerging. Maybe it is the new dominant class." *- McKenzie Wark, Capital Is Dead: Is This Something Worse?* {% cite warkCapitalDeadThis2021 %} 

Data companies --- most major companies --- need to store and maintain massive collections of heterogeneous data across their byzantine hierarchies of executives, managers, and workers. This gigantic haunted ball of data is not just a tool, but the *substance* of the company. A data company persists by exploiting the combinatorics of its data hoard, spinning off new platforms that in turn maintain and expand access to data by creating captive data subjects[^fbgraph]. As it expands, a conglomerate will acquire many new sources and modalities of data and need to integrate them with its existing data. 

Knowledge graphs are particularly well suited for this "data integration" problem. A full technical description is out of scope here, but briefly: traditional relational database systems can be very difficult to modify and refactor, and that difficulty increases the larger and more complex a database is[^etsydb]. One has to be design the structure of the anticipated data in advance, and the abstract schematic structure of the data is embedded in how it is stored and accessed. It is particularly difficult to do unanticipated "long range" analyses where very different kinds of data are analyzed together. 

In contrast, merging graphs is more straightforward[^integration] {% cite enterpriseknowledgegraphfoundationKnowledgeGraphIndustry2022 schenkerNewReportDetails2021 sequedaDesigningBuildingEnterprise2021 segaranTwophaseConstructionData2020 natarajanGraphKnowledgeGraph %} - the data is just triplets, so in an idealized case[^notmagic] it is possible to just concatenate them and remove duplicates (eg. for a short example, see {% cite allemangMergingDataGraphs2022 allemangMergingTablesHard2022 %}). The graph can be operated on locally, with more global coordination provided by ontologies and schemas, which themselves have a graph structure {% cite villazon-terrazasKnowledgeGraphFoundations2017 %}. Discrepancies between graphlike schema can be resolved by, you guessed it, making more graph to describe the links and transformations between them. Long-range operations between data are part of the basic structure of a graph - just traverse nodes and edges until you get to where you need to go - and the semantic structure of the graph provides additional constraints to that traversal. Again, a technical description is out of scope here, graphs are not magic, but they are well-suited to merging, modifying, and analyzing large quantities of heterogeneous data. 

Another way of looking at the capacity for heterogeneity in triplet graphs is by thinking of links as statements:

> One person may define a `vehicle` as having a `number of wheels` and a `weight` and a `length`, but not foresee a `color`. This will not stop another person making the assertion that a given car is `red`, using the color vocabulary from elsewhere. {% cite berners-leeWhatSemanticWeb1998 %}

So if you are a data broker, and you just made a hostile acquisition of another data broker who has additional surveillance information to fill out for the people in your existing dataset, you can just stitch those new properties on like a fifth arm on your nightmarish data frankenstein.

What does this look like in practice? While in a bygone era Elsevier was merely a rentier holding publicly funded research hostage for profit, its parent company RELX is paradigmatic of the transformation of a more traditional information rentier into a sprawling, multimodal surveillance conglomerate (see {% cite lamdanDataCartelsCompanies2023 %}). RELX proudly describes itself as a gigantic haunted graph of data:

> Technology at RELX involves creating actionable insights from big data – large volumes of data in different formats being ingested at high speeds. We take this high-quality data from thousands of sources in varying formats – both structured and unstructured. We then extract the data points from the content, link the data points and enrich them to make it analysable. Finally, we apply advanced statistics and algorithms, such as machine learning and natural language processing, to provide professional customers with the actionable insights they need to do their jobs.
>
> We are continually building new products and data and technology platforms, re-using approaches and technologies across the company to create platforms that are reliable, scalable and secure. **Even though we serve different segments with different content sets, the nature of the problems solved and the way we apply technology has commonalities across the company.** {% cite relxAnnualReport20222023 %}

![Alt Text: A diagram from RELX's 2022 Annual report titled "Delivering To Customers In A Single Point of Execution." The graph is a funnel from left to right, taking in data sources (Public records, Contributory, Licenses, Proprietary), cleaning them, standardizing them, and then relating and analyzing them. The narrow end of the funnel then expands to a series of services (Batch services, Real-Time API services, Visualization integration) illustrating that once the data has been cleaned then it is possible to create a number of derivative platforms off of them. Beneath the graph are four bullet point lists. Unstructured and structured content: Hundreds of thousands of sources, Billions of device and asset identities, Hundreds of millions of records added daily. Big data platforms: Grid computing with low-cost servers, Linking algorithms that generate high precision and recall, Machine learning algorithms to cluster, link, and learn from the data, High speed data ingestion, recall, and processing, rapid development cycles. Analysis applications: Patented algorithms, Predictive modeling, Machine learning and artifical intelligence. Customer single point of execution: Modular product suites, Flexible delivery platforms](/surveillance-graphs/assets/img/RELX_Pipeline_2022.png)
*In its 2022 Annual Report, RELX describes its business model as ingesting large quantities of data, linking them together, and deriving platforms from them. {% cite relxAnnualReport20222023 %}*

While to any individual market segment or class of customers RELX and its subsidiaries might look like a portfolio of separate platforms and applications, one can only make sense of the company by thinking of each of them as a view on an interconnected graph of data[^RELXStrugs]. Each additional source of data, either by acquiring new companies or by expanding their existing control of informational access points has the potential to create some combinatorically new set of opportunities for new platforms.

For example, RELX is able to gather surveillance data on researcher attention data through the tracking in its ScienceDirect and Mendeley platforms. It also collects a large amount of chemical data through its control of scientific publishing that it rents access to on its [Reaxys](https://www.elsevier.com/en-gb/solutions/reaxys) platform, which is supplemented by its LexisNexis PatentSight database of patents. So far so normal.

What about the other sides of the multisided market? RELX is able to combine these and other data sources into new product. For pharmaceutical R&D companies, their bespoke [Drug Design Optimization](https://web.archive.org/web/20211207070524/https://www.elsevier.com/solutions/professional-services/drug-design-optimization) services advertise being able to use chemical, disease, and literature-based data to generate a priority list of potential therapeutic targets and drugs, as well as provide "competitive intelligence" about which targets are currently being studied, presumably identified from their ownership of the scientific literature coupled with surveillance data. Since clinicians don't trust pharmaceutical advertisements {% cite elsevierMakingMedicalInformation2021 %}, Elsevier uses its position as a perceived neutral third party to repackage advertisements as informational systems {% cite elsevierRethinkClincalContent2020 %}, "journal-branded webinars," as well as a number of other avenues via its "[360 degree advertising solutions](https://web.archive.org/web/20211111211058/https://www.elsevier.com/advertising-reprints-supplements/advertising)" catalogue. So, by combining several data sources and platforms, Elsevier is able to offer pharmaceutical companies recommendations for candidate drugs above and beyond what would be possible with chemical information alone and then advertise their drugs directly to doctors. 

Derivative platforms beget derivative platforms. Its integration into clinical systems by way of reference material is growing to include [electronic health record](https://web.archive.org/web/20230307020432/https://www.elsevier.com/en-gb/clinical-solutions/clinical-practice) (EHR) systems, and they are "developing clinical decision support applications [...] leveraging [their] proprietary health graph" {% cite relxAnnualReport20222023 %}. Similarly, their integration into Apple's watchOS to track medications indicates their interest in directly tracking personal medical data. 

That's all within biomedical sciences, but RELX's risk division also provides "comprehensive data, analytics, and decision tools for [...] life insurance carriers" {% cite relxAnnualReport20222023 %}, so while we will never have the kind of external visibility into its infrastructure to say for certain, it's not difficult to imagine combining its diverse biomedical knowledge graph with personal medical information in order to sell risk-assessment services to health and life insurance companies. LexisNexis has personal data enough to serve as an "integral part" of the United States Immigrations and Customs Enforcement's (ICE) arrest and deportation program {% cite biddleLexisNexisProvideGiant2021 biddleICESearchedLexisNexis2022 %}, including dragnet [location data](https://web.archive.org/web/20230308034123/https://risk.lexisnexis.com/products/accurint-trax) {% cite lexisnexisrisksolutionsAccurintTraX %}, [driving behavior data](https://risk.lexisnexis.com/products/telematics-ondemand) from internet-connected cars {% cite lexisnexisrisksolutionsTelematicsOnDemand %}, and [payment and credit data](https://risk.lexisnexis.com/products/threatmetrix) as just a small sample from its large [catalogue](https://web.archive.org/web/20230308034302/https://www.lexisnexis.com/pdf/AccurintForLegalProfessionals/24.pdf) {% cite lexisnexisrisksolutionsAccurintLegalProfessionals2022 %} of data [aggregated and linked](https://risk.lexisnexis.com/our-technology/lexid) into comprehensive profiles {% cite lexisnexisrisksolutionsLexID %}. The contemporary knowledge graph-powered surveillance conglomerate gains its versatility precisely from its ability to span many unrelated domains and deploy new platforms as opportunities present themselves. As new data sources are acquired, the combinatorics of possible surveillance products correspondingly explode.

This pattern is true across the information industry {% cite sequedaDesigningBuildingEnterprise2021 %}. A handful of representatives from Microsoft, Google, Facebook, eBay, and IBM describe some elements of each of their knowledge graphs in a 2019 paper {% cite noyIndustryscaleKnowledgeGraphs2019 %}. Each has different scopes, applications, and interaction with the other data and processing infrastructure at the company, but all emphasize the ability for their knowledge graphs to accomodate change, heterogeneity, conflicting data, inference, and facilitate work by distributed teams due to their self-documenting and modular nature. Neo4j, developers of an eponymous graph database library,  describes in one [case study](https://neo4j.com/case-studies/us-army/) among its [hundreds of customers](https://neo4j.com/customers/) how the U.S. Army uses its "connected data" to track its equipment and estimate the cost of some new exploratory imperialism {% cite neo4jNeo4jArmyCase2021 %}. An analysis of Palantir's hundreds of patents for knowledge graph technology (eg. {% cite cohenSystemMethodSharing2015 mathuraAutomatedDatabaseAnalysis2017 yousafSystemsMethodsUser2018 knudsonSystemsMethodsAnnotating2021 %}) describes its ambitions for its knowledge graph:

> There is evidence [...] that Palantir has infrastructural aspirations to become a general classification system for data integration [...] that can be tailored into a universal knowledge graph. [...] Palantir similarly imagines a world where its platform might serve as a “shadow” universal knowledge graph for governments, industries, and organizations. {% cite iliadisSeerSeenSurveying2022 %}


**closing ideas**
- enclosure of tech
- private ontologies - something that 'they do' and 'we receive'
- the criticism of 'what does a universal ontology mean' doesn't matter when what you're doing is repackaging it into a product.
- but what could come is a hybrid...the notion of peer production of knowledge graphs is a trap.
	- crowdsourcing is literally a euphemism for harvesting other ppls work. people building something together means somethign entirely different when they don't own it.



**what does this look like in practice? what kind of horrors does this spawn???? examples from RELX, palantir, neo4j army**

- pharmaceutical interaction graphs & general life sciences application
- recommendation systems
- fraud detection
- social media graphs
- justice & incarceration 
- army {% cite neo4jNeo4jArmyCase2021 %}







- The change in character is one of 'public information present on the web in the same medium that we use it in, I have tools to be able to directly interact with and understand the data, maybe by way of some automated reasoning agent that I control, but whatevers. -> something where corporations sit on their knowledge graph as *the thing* that they actually are as a business - their structured collection of informatino, the ability to collect and relate adn link it together and repackage it in improbable ways s.t. you always have some new information product to 

---

**THIS SECTION: Knowledge graphs as the corporatization of semantic web, culmination of the platformatization logic. cut to next section with a nod to the development of schema.org, why would organizations like this share data/given the focus on proprietariness, what is in it for them with seemingly open standards that might enable others?**

--

**Move this probably up before switching to talking about public and private?**
The competition between explicitly structuring the web and searching through it, and the role of Google specifically is not an incidental example among many, but instrumental to the development of the web as we know it (see Ben Tarnoff's excellent "Internet for the People" for a fuller account {% cite tarnoffInternetPeopleFight2022 %}). 

What do knowledge graphs give us that the older platform models didn't? the ability to link huge amounts of heterogeneous data, which dovetails with the emergence of the platform -> surveillance model

- Also schema.org starting in 2012 - exemplary of how 'open' efforts lead ultimately to greater dominance. 

- and so that transitions to the question at hand: the turn from strictly private corporate knowledge graphs to ones that can be broadly interoperable and extend to many more domains of life... it mirrors the way that "openness" via schema.org allowed them to consolidate a whole set of processes... so why might we care about government knowledge graphs...

This gives clarity to what exactly is an information business, what is 'good' for it - which kinds of openness are good and which are bad. because you want to be able to be able to maximize interfacial contact with indivudual people - sell ads, gather information - 
the interoperability of a bunch of knowledge graphs is good. you imagine your competitors just some other graph you might want to acquire some day. 
Through the alchemy of informational capitalism, you want to be able to rent and loan your graph out in pieces to competitors that aren't competing with you on a particular domain so that they might fight your other competitors on a different domain. 
it's like ultracapitalism where the capital is just infinitely fungible, but it's these graphs + compute!!! 
interoperability is not a threat because ultimately it's designed around a "graph + compute" anyway, so even if you could get your hands on the graph, you wouldn't be able to use it.


- Use in industry
- Differentiation from prior models of cloud computing and storage
- Why in particular were they adopted by google et al?
- Google's KG in particular:
	- {% cite juelvangEthicsGoogleKnowledge2013 %} - since its creation, people have been questioning the role of google as arbiter or all knowledge. In fact this really freaking pissed off wikipedia and is sort of indicative of the general pillaging of the commons that this kind of business model represent
- Other companies KGs {% cite noyIndustryscaleKnowledgeGraphs2019 %}
	- Microsoft:
		- Bing
		- Academic Graph
		- LinkedIn Graph
	- Facebook: {% cite weaverFacebookLinkedData2013 %}
- Natural counterpart to sprawling surveillance
	- This is exactly why they will be so powerful for our kind of generalized digital infrastructure we want to build: the corporate platforms have identified a tool that can be used to run the world's information. 
	- Eg relx. lots of information acquired over time from a lot of different locations, needs to be able to be reduced to some unified system, and graphs work at the level of propositions and assertions that can then be traversed and resolved. 
	- You also end up having a lot of multidomain data that could refer to the same entity, but in ways that were unanticipated by the initial schema for the object
	- unlike traditional relational databases, this is no problem for a graph. 
	- This also can work in a common interface: you get really dramatically different kinds of information for different kinds of objects, including different kinds of actions that can be taken, but those can be represented in a common format on a graph. 
	- Inference: you can then make logical inferences based on the networked information, this is a natural thing to want to do for search engines, as you are able to "fill in" information that isn't explicitly encoded in the database. Inference also is coproductive with the evolution into chatbots - if you provide some information about some flight you want to take, then the graph can know what information hasn't beeen provided in a complex concept. 

- What makes KGs special:
	- Explicit algo + KG model - entity oriented search: {% cite balogEntityOrientedSearch2018 %} 
	- KGs are best for heterogeneous data: {% cite ceravoloBigDataSemantics2018 %} and specifically in the business case: {% cite chaudhriKnowledgeGraphsIntroduction2022 %}
	- Specific role of standards efforts like Schema.org - where openness is strategically used in order to 


> While Palantir may claim otherwise, we believe there is enough evidence in these patents to suggest that Palantir has an internal knowledge base or knowledge graph for labeling entities in the world, along with their attributes and relationships, and that these occluded resources are not fully released to users, but that users are subject to them in using the products. {% cite iliadisSeerSeenSurveying2022 %}

- KGs are the dominant mode of integrating heterogeneous data: {% cite sequedaDesigningBuildingEnterprise2021 azziniAdvancesDataManagement2021 allemangMergingDataGraphs2022 %}

- Yes, wikidata exists, but it's extremely carefully managed with the intention of creating "facts" that don't embarass google {% cite chahOKGoogleWhat2018 %}
