### Knowledge Graphs: Panoptica

In 2010 Google acquired Metaweb and its publicly-edited Semantic Web database Freebase, and in 2012 repackaged it and the ideas of Linked Data as what it called a **Knowledge Graph** --- the third era of the Semantic Web {% cite singhalIntroducingKnowledgeGraph2012 iainFreebaseDeadLong2016 %}. Freebase only made up part of it, and the full extent of Google's Knowledge Graph are unknown, but its most visible impact are the factboxes that present structured information about the subjects of searches - like biographical information in a search for a person - or the different widgets for contextual interaction - like being able to make a restaurant reservation from the serach page {% cite noyIndustryscaleKnowledgeGraphs2019 %}. Knowledge Graphs still share the same underlying structure --- triplet graphs with ontologies --- even if they occupy a broader space of implementations and technologies. What differs is the context and intended use: the "worldview" of the knowledge graph.

Beyond the obvious product-level features it supports, Google's acquisition of Freebase and the structure of its Knowledge Graph represent at least two deeper shifts in the trajectory of the Semantic Web and the broader internet: the privatization of technologies with initially liberatory aspirations, and an early template of the sprawling, surveillance-driven information conglomerate we know and love today. 

Like the radical nature of linking on the web, it's difficult to remember that the web as surveillance apparatus thinly veiled as the five or so remaining websites was not inevitable. The pre-dotcom bust internet of the 90's and early 2000's was far from the commercialized wasteland we know today. Ed Horowitz, CEO of Viacom explained in 1996: "The Internet has yet to fulfill its promise of commercial success. Why? Because there is no business model" {% cite tarnoffInternetPeopleFight2022 %}. Google's AdWords being a defining moment in the development of surveillance capitalism is a story already told {% cite zuboffAgeSurveillanceCapitalism2019 %}: taking advantage of the need for search generated by the disorganization of the web, AdWords turned personal search data into a profit vector by selling targeted space in the results.

The significance of the relationship between search, the semantic web, and what became knowledge graphs is less widely appreciated. The semantic web was initially an alternative to monolithic search engine platforms - or, more generally, to platforms in general {% cite berners-leeSociallyAwareCloud2009 %}. It imagined the use of triplet links and shared ontologies at a protocol level as a way of organizing the information on the web into a richly explorable space: rather than needing to rely on a search bar, one could traverse a structured graph of information {% cite berners-leeLinkedData2006 berners-leeGoalsHumanDataInterface2010 %} to find what one needed without mediation by a third party.

Instead, the form of of the semantic web that emerged as "Knowledge Graphs" flipped the vision of a free and evolving internet on its head. The mutation from "Linked Open Data" {% cite berners-leeLinkedData2006 %} to "Knowledge Graphs" is a shift in meaning from a public and densely linked web of information from many sources to a proprietary information store used to power derivative platforms and services. The shift isn't quite so simple as a "closure" of a formerly open resource --- we'll return to the complex role of openness in a moment. It is closer to an *en*closure, a *domestication* of the dream of the Semantic Web. A dream of a mutating, pluralistic space of communication, where we were able to own and change and create the information that structures our digital lives was reduced to a ring of platforms that give us precisely as much agency as is needed to keep us content in our captivity. Links that had all the expressive power of utterances, questions, hints, slander, and lies were reduced to mere facts. We were recast from our role as *people* creating a digital world to *consumers* of subscriptions and services. The artifacts that we create for and with and between each other as the substance of our lives online were yoked to the acquisitive gaze of the knowledge graph as *content* to be mined. We vulgar commoners, we data subjects, are not allowed to touch the graph --- even if it is built from our disembodied bits.

The same technologies, with minor variation, that were intended to keep the internet free became emblematic of and coproductive with the surveillance/platform model that has enclosed it. Beyond Google, knowledge graphs are an elemental part of the contempory information economy. Banks, militaries, governments, life science corporations, journalists, everyone is using knowledge graphs {% cite neo4jNeo4jCustomers enterpriseknowledgegraphfoundationKnowledgeGraphIndustry2022 %}. Their ubiquity is not an accident, one of many possible data systems that could have fit the bill, but reflects and reinforces basic patterns of the information economy and the corporations within it. 

What makes knowledge graphs so special? It turns out that semantic web technologies, designed to accomodate the infinitely heterogeneous, multiscale nature of free and unmediated social structuring of information are also quite useful for the indefinitely expanding dragnet of data collection that defines the operation of contemporary capitalism:

> "If one takes a look at the top Fortune 500 companies, it is surprising how many of them are really in the information business. I don’t just mean the technology and telecommunication companies like Apple or Google or Verizon or Cisco or the drug companies like Pfizer. One could also think of the big banks as a subset of the vectoralist class rather than as “finance capital.” They too are in the information asymmetry business. And as we learned in the 2008 crash, even the car companies are in the information business—they made more money from car loans than cars. The military—industrial sector is also in the information business. The companies that appear to sell actual things, like Nike, are really in the brand business. Walmart and Amazon compete with different models of the information logistics business. Even the oil companies are in part at least in the information-about-the-geology-of-possible-oil-deposits business. Perhaps the vectoralist class is no longer emerging. Maybe it is the new dominant class." *- McKenzie Wark, Capital Is Dead: Is This Something Worse?* {% cite warkCapitalDeadThis2021 %} 

Data companies --- most major companies --- need to store and maintain massive collections of heterogeneous data across their byzantine hierarchies of executives, managers, and workers. This gigantic haunted ball of data is not just a tool, but the *substance* of the company. A data company persists by exploiting the combinatorics of its data hoard, spinning off new platforms that in turn maintain and expand access to data by creating captive data subjects[^fbgraph]. As it expands, a conglomerate will acquire many new sources and modalities of data and need to integrate them with its existing data. 

Knowledge graphs are particularly well suited for this "data integration" problem. A full technical description is out of scope here, but briefly: traditional relational database systems can be very difficult to modify and refactor, and that difficulty increases the larger and more complex a database is[^etsydb]. One has to be design the structure of the anticipated data in advance, and the abstract schematic structure of the data is embedded in how it is stored and accessed. It is particularly difficult to do unanticipated "long range" analyses where very different kinds of data are analyzed together. 

In contrast, merging graphs is more straightforward[^integration] {% cite chaudhriKnowledgeGraphsIntroduction2022 enterpriseknowledgegraphfoundationKnowledgeGraphIndustry2022 schenkerNewReportDetails2021 sequedaDesigningBuildingEnterprise2021 
azziniAdvancesDataManagement2021 segaranTwophaseConstructionData2020 ceravoloBigDataSemantics2018 natarajanGraphKnowledgeGraph %} - the data is just triplets, so in an idealized case[^notmagic] it is possible to just concatenate them and remove duplicates (eg. for a short example, see {% cite allemangMergingDataGraphs2022 allemangMergingTablesHard2022 %}). The graph can be operated on locally, with more global coordination provided by ontologies and schemas, which themselves have a graph structure {% cite villazon-terrazasKnowledgeGraphFoundations2017 %}. Discrepancies between graphlike schema can be resolved by, you guessed it, making more graph to describe the links and transformations between them. Long-range operations between data are part of the basic structure of a graph - just traverse nodes and edges until you get to where you need to go - and the semantic structure of the graph provides additional constraints to that traversal. Again, a technical description is out of scope here, graphs are not magic, but they are well-suited to merging, modifying, and analyzing large quantities of heterogeneous data. 

Another way of looking at the capacity for heterogeneity in triplet graphs is by thinking of links as statements:

> One person may define a `vehicle` as having a `number of wheels` and a `weight` and a `length`, but not foresee a `color`. This will not stop another person making the assertion that a given car is `red`, using the color vocabulary from elsewhere. {% cite berners-leeWhatSemanticWeb1998 %}

So if you are a data broker, and you just made a hostile acquisition of another data broker who has additional surveillance information to fill out for the people in your existing dataset, you can just stitch those new properties on like a fifth arm on your nightmarish data frankenstein.

What does this look like in practice? While in a bygone era Elsevier was merely a rentier holding publicly funded research hostage for profit, its parent company RELX is paradigmatic of the transformation of a more traditional information rentier into a sprawling, multimodal surveillance conglomerate (see {% cite lamdanDataCartelsCompanies2023 %}). RELX proudly describes itself as a gigantic haunted graph of data:

> Technology at RELX involves creating actionable insights from big data – large volumes of data in different formats being ingested at high speeds. We take this high-quality data from thousands of sources in varying formats – both structured and unstructured. We then extract the data points from the content, link the data points and enrich them to make it analysable. Finally, we apply advanced statistics and algorithms, such as machine learning and natural language processing, to provide professional customers with the actionable insights they need to do their jobs.
>
> We are continually building new products and data and technology platforms, re-using approaches and technologies across the company to create platforms that are reliable, scalable and secure. **Even though we serve different segments with different content sets, the nature of the problems solved and the way we apply technology has commonalities across the company.** {% cite relxAnnualReport20222023 %}

![Alt Text: A diagram from RELX's 2022 Annual report titled "Delivering To Customers In A Single Point of Execution." The graph is a funnel from left to right, taking in data sources (Public records, Contributory, Licenses, Proprietary), cleaning them, standardizing them, and then relating and analyzing them. The narrow end of the funnel then expands to a series of services (Batch services, Real-Time API services, Visualization integration) illustrating that once the data has been cleaned then it is possible to create a number of derivative platforms off of them. Beneath the graph are four bullet point lists. Unstructured and structured content: Hundreds of thousands of sources, Billions of device and asset identities, Hundreds of millions of records added daily. Big data platforms: Grid computing with low-cost servers, Linking algorithms that generate high precision and recall, Machine learning algorithms to cluster, link, and learn from the data, High speed data ingestion, recall, and processing, rapid development cycles. Analysis applications: Patented algorithms, Predictive modeling, Machine learning and artifical intelligence. Customer single point of execution: Modular product suites, Flexible delivery platforms](/surveillance-graphs/assets/img/RELX_Pipeline_2022.png)
*In its 2022 Annual Report, RELX describes its business model as ingesting large quantities of data, linking them together, and deriving platforms from them. {% cite relxAnnualReport20222023 %}*

While to any individual market segment or class of customers RELX and its subsidiaries might look like a portfolio of separate platforms and applications, one can only make sense of the company by thinking of each of them as a view on an interconnected graph of data[^RELXStrugs]. Each additional source of data, either by acquiring new companies or by expanding their existing control of informational access points has the potential to create some combinatorically new set of opportunities for new platforms.

For example, RELX is able to gather surveillance data on researcher attention data through the tracking in its ScienceDirect and Mendeley platforms. It also collects a large amount of chemical data through its control of scientific publishing that it rents access to on its [Reaxys](https://www.elsevier.com/en-gb/solutions/reaxys) platform, which is supplemented by its LexisNexis PatentSight database of patents. So far so normal.

What about the other sides of the multisided market? RELX is able to combine these and other data sources into new product. For pharmaceutical R&D companies, their bespoke [Drug Design Optimization](https://web.archive.org/web/20211207070524/https://www.elsevier.com/solutions/professional-services/drug-design-optimization) services advertise being able to use chemical, disease, and literature-based data to generate a priority list of potential therapeutic targets and drugs, as well as provide "competitive intelligence" about which targets are currently being studied, presumably identified from their ownership of the scientific literature coupled with surveillance data. Since clinicians don't trust pharmaceutical advertisements {% cite elsevierMakingMedicalInformation2021 %}, Elsevier uses its position as a perceived neutral third party to repackage advertisements as informational systems {% cite elsevierRethinkClincalContent2020 %}, "journal-branded webinars," as well as a number of other avenues via its "[360 degree advertising solutions](https://web.archive.org/web/20211111211058/https://www.elsevier.com/advertising-reprints-supplements/advertising)" catalogue. So, by combining several data sources and platforms, Elsevier is able to offer pharmaceutical companies recommendations for candidate drugs above and beyond what would be possible with chemical information alone and then advertise their drugs directly to doctors. 

Derivative platforms beget derivative platforms. Its integration into clinical systems by way of reference material is growing to include [electronic health record](https://web.archive.org/web/20230307020432/https://www.elsevier.com/en-gb/clinical-solutions/clinical-practice) (EHR) systems, and they are "developing clinical decision support applications [...] leveraging [their] proprietary health graph" {% cite relxAnnualReport20222023 %}. Similarly, their integration into Apple's watchOS to track medications indicates their interest in directly tracking personal medical data. 

That's all within biomedical sciences, but RELX's risk division also provides "comprehensive data, analytics, and decision tools for [...] life insurance carriers" {% cite relxAnnualReport20222023 %}, so while we will never have the kind of external visibility into its infrastructure to say for certain, it's not difficult to imagine combining its diverse biomedical knowledge graph with personal medical information in order to sell risk-assessment services to health and life insurance companies. LexisNexis has personal data enough to serve as an "integral part" of the United States Immigrations and Customs Enforcement's (ICE) arrest and deportation program {% cite biddleLexisNexisProvideGiant2021 biddleICESearchedLexisNexis2022 %}, including dragnet [location data](https://web.archive.org/web/20230308034123/https://risk.lexisnexis.com/products/accurint-trax) {% cite lexisnexisrisksolutionsAccurintTraX %}, [driving behavior data](https://risk.lexisnexis.com/products/telematics-ondemand) from internet-connected cars {% cite lexisnexisrisksolutionsTelematicsOnDemand %}, and [payment and credit data](https://risk.lexisnexis.com/products/threatmetrix) as just a small sample from its large [catalogue](https://web.archive.org/web/20230308034302/https://www.lexisnexis.com/pdf/AccurintForLegalProfessionals/24.pdf) {% cite lexisnexisrisksolutionsAccurintLegalProfessionals2022 %} of data [aggregated and linked](https://risk.lexisnexis.com/our-technology/lexid) into comprehensive profiles {% cite lexisnexisrisksolutionsLexID %}. The contemporary knowledge graph-powered surveillance conglomerate gains its versatility precisely from its ability to span many unrelated domains and deploy new platforms as opportunities present themselves. As new data sources are acquired, the combinatorics of possible surveillance products correspondingly explode.

This pattern is true across the information industry {% cite sequedaDesigningBuildingEnterprise2021 %}. A handful of representatives from Microsoft, Google, Facebook, eBay, and IBM describe some elements of each of their knowledge graphs in a 2019 paper {% cite noyIndustryscaleKnowledgeGraphs2019 %}. Each has different scopes, applications, and interaction with the other data and processing infrastructure at the company, but all emphasize the ability for their knowledge graphs to accomodate change, heterogeneity, conflicting data, inference, and facilitate work by distributed teams due to their self-documenting and modular nature. Neo4j, developers of an eponymous graph database library,  describes in one [case study](https://neo4j.com/case-studies/us-army/) among its [hundreds of customers](https://neo4j.com/customers/) how the U.S. Army uses its "connected data" to track its equipment and estimate the cost of some new exploratory imperialism {% cite neo4jNeo4jArmyCase2021 %}. An analysis of Palantir's hundreds of patents for knowledge graph technology (eg. {% cite cohenSystemMethodSharing2015 mathuraAutomatedDatabaseAnalysis2017 yousafSystemsMethodsUser2018 knudsonSystemsMethodsAnnotating2021 %}) describes its ambitions for its knowledge graph:

> There is evidence [...] that Palantir has infrastructural aspirations to become a general classification system for data integration [...] that can be tailored into a universal knowledge graph. [...] Palantir similarly imagines a world where its platform might serve as a “shadow” universal knowledge graph for governments, industries, and organizations. {% cite iliadisSeerSeenSurveying2022 %}

Knowledge graphs *as a technology* - like all technologies - are not intrinsically unethical. It is the structure of the capital-K capital-G Knowledge Graph *as a concept* with its *context* that is pathological. They represent the historical trajectory of semantic web ideas and technologies from something that we are intended to use and create directly into privately held data that we can only interact with through platforms. They are coproductive with the corporate and technical structure of surveillance capitalism, facilitating conglomerates that gobble up as many platforms and data sources as possible to stitch them into an expanding, heterogeneous graph of data. We will return to the underlying ideology of the knowledge graph and an alternative in the final section.

In particular, it is their "graph plus compute" structure - where some underlying graph of data is coupled with a set of algorithms and interfaces to view it - that is necessary to understand some of the more counterintuitive motivations of surveillance conglomerates. This structure complicates questions of "openness" versus "proprietariness," one of the deepest loci of criticism of the platformatized web, and provides a different lens on ostensibly "open" or "public" knowledge graph-based infrastructure projects.
