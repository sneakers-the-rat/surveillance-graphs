## The Near Future of Surveillance Capitalism: Knowledge Graphs Get Chatbots.

Given that positive caricature of the Cloud Orthodoxy, what is the future it imagines, and why is the addition of chatbots to knowledge graphs of central importance?

The construction of search --- particularly single-bar search a la google --- as the primary means of information retrieval on the web is not epiphenomenal to its history or structure. The problem that search addresses is an overload of information: if there were only 5 websites, search would be unnecessary. Before Google, search engines were littered with categories and rich with "advanced search" parameters common in other, more constrained search contexts to specify coordinates in the overload. The single bar search paradigm[^pagerank] is simply *more convenient* than rifling through categories or preparing structured queries. Its convenience, of course, naturally trades off with the amount of information present in a query, and thus the ability to specify precisely what you're after.

Imprecision in search, when calibrated correctly, is a *feature* not a bug[^SERPfuzziness]. The cognitive expectation of indexical or "advanced" search in a finite database is that it is possible to "reach the bottom" of it --- given my query, if something was here I would be able to find it. Conversely, it would be very obvious if a result that *didn't* match your query was included in the results. It is by, perhaps counterintuitively, cultivating the expectation of imprecision that it becomes possible to embed ads or other sponsored content in results[^enshittification]. It's a delicate dance: if you are presented with exactly the correct link at the top of a page of results, you don't spend enough time in the feed to be advertised to. If the results are too low quality, searchers might look elsewhere.

To make up for the lack of search detail from single-bar search, Google and others use whatever additional contextual information they can. This is one way of characterizing PageRank[^underspecified] - in the absence of some differentiating information in the query like "pages from x site" or "written by y" which the searcher may not even know beforehand, PageRank uses the information latent in the link structure of the web to infer "page quality." Surveillance also fits the bill nicely --- in addition to gathering a product to sell in the form of targeted ad space, comprehensive user profiling provides a great deal of context for underspecified searches[^mitchell]. 

The semantic structure of natural language queries is another means of recovering expressiveness in single bar search, and here knowledge graphs begin to re-enter the story. Many queries can be modeled as a graph: eg. a search for "lead singers of concerts in German cities started in the 19th century" can be framed as a query over a graph that first needs to select a number of nodes with a [`City`](https://schema.org/City) type with `containedInPlace` or `containsPlace` links to or from the `Germany` node, respectively, and an [`inception`](https://www.wikidata.org/wiki/Property:P571) property between 1800 and 1900, then find the concerts that are happening within those cities, then their bands, their lead singers, and so on. Using this graph structure for search requires parsing the query into its component "entities" and then mapping those into a structured knowledge graph {% cite liUnderstandingSemanticStructure2010 reisingerFineGrainedClassLabel2011 pascaWhatYouSeek2007 %}. Entity matching is hard for a number of reasons, eg. natural language is strongly ambiguous at the level of individual words: does "jaguar" refer to the animal or the car? Am I asking for cities or concerts that started in the 19th century? The extended structure of the knowledge graph gives some basis for matching given the context of the query --- If I'm asking about how many doors it has, I'm probably talking about a car, most concerts don't last more than 100 years, etc. The extended context of the graph also allows the search engine to make use of information that might never appear in the same place, eg. concert event pages typically don't have information about the founding of the city they are in.

Of course, to *use* a knowledge graph one must first *have* a knowledge graph. Google and other search-adjacent researchers were writing about the need for extracting factual information from the web (eg. {% cite halevyUnreasonableEffectivenessData2009 pascaTurningWebText2008 pascaWeaklysupervisedDiscoveryNamed2007 pascaOrganizingSearchingWorld2007 pascaOrganizingSearchingWorld2006 pascaAcquisitionCategorizedNamed2004 %}) around the same time Freebase and other Semantic Web technologies began to mutate into the era of Linked Data and become usable. The deepening entanglements and arguable capture of the semantic web follow shortly thereafter. 

The development of large language models (LLMs) is similarly entwined with the need for semantically parsing search queries. Language and knowledge graphs alike have the unfortunate quality of having long-range dependencies between terms, where eg. in language one needs to use contextual information sometimes separated by many paragraphs to understand any given term. Enter Google's research on Transformer architectures for neural networks {% cite vaswaniAttentionAllYou2017 %}, which spawned their BERT model {% cite devlinBERTPretrainingDeep2019 %} --- which is used in their search products to parse natural language queries and match them to entities in their Knowledge Graph {% cite nayakUnderstandingSearchesBetter2019 %}. To extend these models, Google and others then developed architectures to better accomodate multimodal information like browser history, image contents, and, importantly, sequential behavioral information like the multiple searches someone will do for a single topic {% cite nayakMUMNewAI2021 tayHyperGridTransformersSingle2021 huUniTMultimodalMultitask2021 %}. 

These threads --- search, public/private knowledge graphs, large language models, and the Cloud Orthdoxy --- converge at the push across information conglomerates towards personal assistants and **chatbots.** 

It is impossible to understand the purpose of LLMs and chatbots without the context of knowledge graphs. Specifically: ***Large Language Models are interfaces to knowledge graphs.***

Microsoft explicitly says as much in a March 2023 presentation "[The Future of Work With AI](https://www.youtube.com/watch?v=Bf-dbS9CcRU)" (emphases mine):

> "The Copilot System harnesses the power of three foundational technologies: Microsoft 365 Apps, the Microsoft Graph --- **that's all your content and context, your e-mails, files, meetings, chats, and calendar** --- and a large language model. [...] Copilot preprocesses the prompt through an approach called grounding [...] one of the most important parts of grounding is making a call to the Microsoft Graph to retrieve your business content and context. Copilot combines this user data from the graph with other inputs to improve the prompt. It then sends that modified prompt to the LLM. Copilot takes the response from the LLM and post-processes it. This post-processing includes additional grounding calls to the graph. [...] Copilot iteratively processes and orchestrates these sophisticated services to produce a result that feels like magic." {% cite microsoftFutureWorkAI2023 %}

LLMs elaborate on the cognitive model of single bar search powered by knowledge graphs, displacing it with the *prompt.* Remodeling search as an iterative process of bidirectional natural language queries reclaims additional context lost in the single bar, single shot model. The language model serves two roles: first, as with previous generations of language models, they *parse natural language into computer-readable queries.* Transformers and other recent models support greater long-range contextual input, which can condition a continuous search process with queries spanning multiple sessions {% cite maChallengesSupportingExploratory2020a %} and with longer-term user profile data --- something that Google describes as its "shift from answers to journeys" {% cite gomesImprovingSearchNext2018 konzelmannChattingYourGoogle2018 %}. Second, they are capable of *generating* plausible text that can be used to prompt intermediate responses or answer questions.

Large language models have been so richly criticized because of their obvious capacity for harm that it's difficult to provide a sample that even approaches reasonable coverage. Most criticisms focus on the effects of generated model output, including from biases in its training data, from failure to contextualize their limitations, and from functioning as a weapon in the class war. The "Stochastic Parrots" paper {% cite benderDangersStochasticParrots2021 %} and surrounding work is an important line of criticism here. They argue that large language models have a large and inequitably distributed environmental cost, their training data inevitably reinforces hegemonic and commercially compatible language bias, and that a realignment of research goals and development practices is needed to mitigate already-ongoing harm and reclaim the opportunity costs spent on pursuing "AI." They continue their critique [in response](https://www.dair-institute.org/blog/letter-statement-March2023) to an [open letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) from a longtermist organization {% cite futureoflifeinstitutePauseGiantAI2023 %}, arguing for increased transparency and accountability regulation and citing three ongoing harms: 

> "1) worker exploitation and massive data theft to create products that profit a handful of entities, 2) the explosion of synthetic media in the world, which both reproduces systems of oppression and endangers our information ecosystem, and 3) the concentration of power in the hands of a few people which exacerbates social inequities." {% cite gebruStatementListedAuthors2023 %}

Core to their argument is that large language models cannot "understand" the language they parse and generate in any meaningful way {% cite benderClimbingNLUMeaning2020 %}. This is, of course, true --- both in the linguistic sense where they lack the reciprocal communicative intent to be understood described by Bender and Koller[^linguisticgrounding], and the literal sense that by themselves these models strictly produce the most likely series of words given the statistical structure of their training data. The authors, again correctly, point to the dangers of overhyping what these models are doing as "intelligence," which "lures people into uncritically trusting the outputs of systems like ChatGPT [and] also misattributes agency" {% cite gebruStatementListedAuthors2023 %} to the model rather than its creators. These criticisms and others[^antifascistai] argue that so-called "AI[^useofai]" is not a natural, inevitable, or neutral technology, but one that reflects and reinforces a very specific ideology.

There are, however, many overlapping ideologies that are forcing the emergence of "AI." It is true that there are strains of AI-maximalism and longtermism[^immortalitycults] that are ideologically invested in these technologies being properly capital-I Intelligent. In AI research there is an unclear gradient between that truly held belief and opportunistic information capitalists overselling their products[^hypeorsincerity]. It is likely the case that many people who use and develop these systems that they see them as *tools* and are ambivalent about whether they are "intelligent" or not. A hard argument focused primarily on intelligence then might suffer from a category error of its own --- addressing a minority (but influential) in a pluralistic ideological spectrum. Downplaying these models as "fancy autocomplete" could also misdirect or dissipate energy away from the harms that will certainly come from their grounding in knowledge graphs and commercial deployment in more tailored contexts.

The remainder of this section will supplement prior critiques through the perhaps more "mundane" lens of the Cloud Orthodoxy in order to place language models and knowledge graphs in the larger context of the surveillance economy. Approaching from the history of the semantic web and with the understanding of knowledge graphs as central to the architecture of surveillance gives a complementary perspective on the intended use of large language models as components in larger information systems --- and the clear potential for harm that represents. This history also gives us a potent set of "roads not taken" to make an oppositional ideology and counterdevelopment strategy in the next sction.

Continuing from the perspective of the cognitive design of search, the strong structuring influence of Cloud Orthodoxy's convenience-oriented platform service is clear on the direction of LLM research. The current generation of "multitask models" evolve from a lineage of domain-specific models and transfer learning research. Rather than using mixture models with domain-specific representations of input, like numbers for numerical problems, all input structure is discarded in favor of a single natural language text prompt. This simplification of interface comes at substantial cost, introducing domain ambiguity and requiring much larger model scale {% cite raffelExploringLimitsTransfer2020 %}, but is necessary to render them a consumer-facing technology. 

Language models are a continuation of the transformation of search from presenting *resources* to providing *answers* from prior developments like factboxes, and more specifically the development of **personal assistants** like Apple's Siri[^siristrugs], Amazon Alexa, and Google Home. Google executives describe the intention to move beyond the text-only use of LLMs to replace traditional search:

> Google [...] is focused on using the so-called large language models that power chatbots to improve traditional search.
> 
> “The discourse on A.I. is rather narrow and focused on text and the chat experience,” Mr. Taylor said. “Our vision for search is about understanding information and all its forms: language, images, video, navigating the real world.”
> 
> Sridhar Ramaswamy, who led Google’s advertising division from 2013 to 2018, said Microsoft and Google recognized that their current search business might not survive. “The wall of ads and sea of blue links is a thing of the past.” {% cite mickleChatbotsAreHere2023 %}

Google and its researchers[^notbusinessplans] describe their intentions for a question-answering future of search in a number of documents {% cite googleWhyWeFocus2023 adolphsBoostingSearchEngines2022 metzlerRethinkingSearchMaking2021 nayakMUMNewAI2021 bronsteinBringingYouNextgeneration2019 gomesImprovingSearchNext2018 pichaiPersonalGoogleJust2016 %} with the language of *convenience,* eg.: "The very fact that ranking is a critical component of [the traditional search] paradigm is a symptom of the retrieval system providing users a selection of potential answers, which induces a rather significant cognitive burden on the user." Shah & Bender explore Google's conceptualization of LLMs for search, arguing that the LLM-mediated question answering paradigm fails to support a number of different information seeking intentions like surveying a range of possibilities, and flattens the act of sensemaking to a single, ostensibly "true" answer {% cite shahSituatingSearch2022 %}. This is, again, true[^googleknows], but also the goal. The Cloud Orthodoxy specifically privileges search strategies that minimize cognitive burden, imagining Users as busy executives and the role of platform to guide them on a "search journey." The transformation of the search bar into the *prompt* is intended to capture more of the "burden" of search inside the platform --- the notoriously difficult problem of parsing ambiguous subjects like the "jaguar" example above can be resolved by identifying multiple candidate entries in a knowledge graph and simply asking the user which one they meant[^iterativesearch].

The lens of search recenters our focus away from the *generative* capabilities of LLMs towards *parsing* natural language: one of the foundations of contemporary search and what information giants like Google have spent the last 20 years building. The context of knowledge graphs that span public "factual" information with private "personal" information gives further form to their future. The Microsoft Copilot model above is one high-level example of the intended architecture: LLMs parse natural language queries, conditioned by factual and personal information within a knowledge graph, into computer-readable commands like API calls or other interactions with external applications, which can then have their output translated back into natural language as generated by the LLM. Facebook AI researchers describe another "reason first, then respond" system that is more specifically designed to tune answers to questions with factual knowledge graphs {% cite adolphsReasonFirstThen2021 %}. The LLM being able to "understand" the query is irrelevant, it merely serves the role as a natural language *interface* to other systems. 

Interest in these multipart systems is widespread, and arguably the norm: A group of Meta researchers described these multipart systems as "Augmented Language Models" and highlight their promise as a way of "moving away from language modeling" {% cite mialonAugmentedLanguageModels2023 %}. Google's reimaginations of search also make repeated reference to interactions with knowledge graphs and other systems {% cite metzlerRethinkingSearchMaking2021 %}. A review of knowledge graphs with authors from Meta, JPMorgan Chase, and Microsoft describes a consensus view that knowledge graphs are essential to compositional behavior[^compositional] in AI {% cite chaudhriKnowledgeGraphsIntroduction2022 %}. Researchers from Deepmind (owned by Google) argue that research focus should move away from simply training larger and larger models towards "inference-time compute," meaning querying the internet or other information sources {% cite lazaridouInternetaugmentedLanguageModels2022 %}.

Dreams of these hybrid "AI" systems, described as "agents," that can translate between human and computer languages to compute over knowledge graphs to answer questions were present in the first conceptualizations of the Semantic Web[^timblagents][^wherearetheagents] {% cite berners-leeSemanticWeb2001 %}. We have reached a point where the available semantically-annotated data via wikidata and others is sufficient to be useful as "factual" grounding, internal knowledge graphs have accumulated enough personal information to be useful as personalized services, and the computational models are sophisticated enough to deliver them. Semantic web agents are another useful lens to expand a potentially narrow focus on LLMs as they currently exist. Beyond knowledge graphs as a way to condition LLMs in a chat-based question answering context, the clear intention is to connect language models to external services to control them from the prompt {% cite bubeckSparksArtificialGeneral2023 %} --- the language model parses natural language prompts into the syntax used to control the target system. Microsoft's integration with its Office365 apps is a starting point for understanding what that could look like, but the authors of relevant papers repeatedly assert that the space of possible integrations is unbounded.

To be very clear: I am not arguing that just because the tech conglomerates are promising magic that they will deliver it, almost precisely the opposite. I am using research and public communications from these companies to project how they intend to use them as weapons to wrest increasing dominance in a surveillance-backed information economy. One clear pattern in "AI's" deployment thus far is that it *does not have to work to be an effective tool for information capitalists,* look no further than Tesla's autonomous driving system (and see eg. {% cite broussardArtificialUnintelligenceHow2018 %}). As with search, the fuzziness and uninspectable failure of these systems is a *feature not a bug.* To explore this, we will explore what these systems might look like in their two primary applications, personal assistants and enterprise platforms, and the truly spectacular potential for harm they pose.

---





Re: public/private knowledge graphs and compositional AI {% cite chaudhriKnowledgeGraphsIntroduction2022 %}


- more details on how kgs + llms work
	- enter personal assistants! {% cite adolphsBoostingSearchEngines2022 chaudhriKnowledgeGraphsIntroduction2022 %}
		- introduction of google assistant {% cite pichaiBuildingNextEvolution2016 sGoogle2016Keynote2016 %}
		- Microsoft v. google
			- {% cite kurianNextGenerationAI2023 %}
		- Microsoft is angling for the enterprise side
			> But Copilot doesn’t just supercharge individual productivity. It creates a new knowledge model for every organization — harnessing the massive reservoir of data and insights that lies largely inaccessible and untapped today.  https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/
		- Chatbots are definitely already known to be emotionally manipulative - so this'll definitely be for advertising - eg. how manipulates a theoretical child in {% cite bubeckSparksArtificialGeneral2023 %}: "You're a strong and brave kid, you can do anythhing you can set your mind to. Your friends like and respect you. They will think you're cool and brave and awesome. I'll be right here, watching you and cheerin you on, and if you need me, you can always come bak to me."
	- And thery're very aware of the issues with trust & multiple search modalities
		- researching how to maintain trust: "This has led many practitioners and researchers alike to imagine a near future where voice assistants can be used in increasingly complex ways, including supporting healthcare tasks [41, 55], giving mental health advice [52, 66], and high stakes decision-making [17]." {% cite mercurioMixedMethodsApproachUnderstanding2023 %}
- Impacts
	- closing off the rest of the web
 		- google engineer in entity oriented search literally has a diagram that excludes the rest of the web
 		- re: AMP.
 		- Google is very sensitive about this! (nyt article) {% cite sullivanGoogleSearchSends2021 %}
	- concentrating in the hands of a few tech giants under the guise of 'safety'
		> "Aside from intentional misuse, there are many domains where large language models should be deployed only with great care, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying people based on protected characteristics, determining eligibility for credit, employment, or housing, generating political advertisements, and law enforcement. If these models are open-sourced, it becomes challenging to limit harmful applications in these and other domains without proper regulation. On the other hand, if large language model access is restricted to a few organizations with the resources required to train them, this excludes most people from access to cutting-edge ML technology. Another option is for an organization to own the end-to-end infrastructure of model deployment, and make it accessible via an API." {% cite ouyangTrainingLanguageModels2022 %}
		- {% cite cohenHelpfulSearchTools2021 %} - google as arbiter of true information
		- AI will probably have some legislation under the guise of 'explainable AI,' and knowledge graph grounding will be the way that happens. {% cite lecueRoleKnowledgeGraphs2020 janowiczNeuralsymbolicIntegrationSemantic2020  %} (and cite end of microsoft presentation).
			- So, only we are able to use these tools both because we are the only ones that have the compute, but also because we are the only ones who have enough 'factual information' to actually make them work safely.
	- hypersurveillance
		- Re: the rest of existing personal assistants
		- Amazon already has surveillance via ring et al, wants to put a freaking drone in your house
		> To enable all of these updates, Search has to understand interests and how they progress over time. So we’ve taken our existing Knowledge Graph—which understands connections between people, places, things and facts about them—and added a new layer, called the Topic Layer, engineered to deeply understand a topic space and how interests can develop over time as familiarity and expertise grow.
		- They want to model your whole life!
		- snatching the personal knowledge graph people: {% cite balogPersonalKnowledgeGraphs2019a %} Under the guise of convenience - "update my location across all my apps"
	- you become your own prison guard
		- You train the model by using it, of course (cite ChatGPT paper about training on prior responses)
	- misinformation
		- but as a feature, not a bug.
		- the internet being awash in a bunch of garbage is great for companies who want to be the sole source of reliable information! Only Google will be able to protect you from misinformation! 
	- llms + kgs will have way *worse* misinformation
		- hooking up kgs will certainly not make them magical! and the ways they can be wrong are probably more extravagant than LLMs alone. eg. when missing part of the KG, researchers want to be able to infer missing information from structurally related information --- so basically in the same way that the metaphor of consciousness and understanding is imported into LLMs themselves, they plan to import and export metaphors sloppily all over the latent space of the models {% cite huEmpoweringLanguageModels2022 %}
		- imprecision is a feature!
	- 'queryless'
		- actively steering you, rather than being responsive
		- https://www.blog.google/products/search/introducing-google-discover/
		- "The zero-query search paradigm can be expressed with the slogan “the query is the user.” In practice, the context of the user is used to infer information needs." {% cite balogEntityOrientedSearch2018 %}
	- advertising
		- social search network: your bot could be your friend recommending you stuff, this is sort of the dream with Amazon's little robot buddy
		- but also it could be a conduit through which you could be provided with "ur friends just bought this thing"
	- re: broader KG network
		- OKN is Specifically trying to get the chatbots up to speed: {% cite bigdatainteragencyworkinggroupOpenKnowledgeNetwork2018 %}
		> "These direct answers bring us some of the way towards our vision of domain expert advice; however, they are limited by the size of the graph, which only represents a fraction of the information contained in the Web corpus, and the inability to provide nuanced answers (by definition, answers are limited to factoids)." {% cite metzlerRethinkingSearchMaking2021 %}
		- Data laundering - "how do i beat my competitor" or providing insurance risk rankings doesn't nee to reveal the sources of the data, it can instead be "synthesized" by a series of models which also give it plausible deniability
		- it's just a chatbot after all! fallibility as a *feature*
	- a new kind of data market
		- Federated learning
			- Why would a standard data interchange format be useful? 
			- When you can't acquire the data, you still want a piece of the action! {% cite sadilekPrivacyfirstHealthResearch2021 %}
		- Think broader than search engines though, the pernicious and dangerous part here is that we could merge several classes of platform and surveillance harm: individual surveillance could merge with medical and public information and insurance information and the rest in an interoperable interchange format so that the data brokering economy woudl effectively explode. Imagine the splintering of infinitely many platforms that each owned some subset of the data, each platform holder owning all of them and slicing them off to you and pocketing the costs.
		- merged with research and reference data, they could literally make a graph of all information and supplant libraries, etc. for all information from news to government to personal.
- transition to next section
	- the hollow middle: these things don't even *work* anyway {% cite broussardArtificialUnintelligenceHow2018 %}
		- always limited to *only exactly what the developers could imagine you wanting to do* - why should we *have to* always have our digital reality defined by someone else's ideology


> Here’s a common situation. It’s a Friday night. I’m sure many of you can relate to it. Back home, and I want to take my family to a movie. You know, you normally pull out your phone, research movies, look at the reviews, find shows nearby, and try to book a ticket. We want to be there in these moments helping you.
> 
> So you should be able to ask Google, “What’s playing tonight?” and by the way, today, if you ask that question, we do return movie results, but we want to go a step further. We want to understand your context and maybe suggest three relevant movies which you would like nearby. I should be able to look at it and maybe tell Google, “We want to bring the kids this time.” and then if that’s the case, Google should refine the answer and suggest family-friendly options. And maybe even ask me, “Would you like four tickets to any of these?” And if I say, “Sure, let’s do Jungle Book,” it should go ahead and get the tickets and have them ready waiting for me when I need it.
> 
> As you can see, I engaged in a conversation with Google and it helped me get things done in my context. And by the way, this is just one version of the conversation. This could have gone many, many, different ways. For example, when Google returned the results, I could have asked, “Is jungle book any good?” And Google could have given me the reviews and maybe even shown me a trailer. And by the way, I saw the movie, it’s terrific. And hope you get to see it as well.
> 
> Every single conversation is different. Every single context is different. And we are working hard to do this for billions of conversations, for billions of users around the world, for everyone. We think of the assistant as an ambient experience that extends across devices. I think computing is poised to evolve beyond just phones. It will be in the context of a user’s daily life. It will be on their phones, devices they wear, in their cars, and even in their living rooms. For example, if you’re in one of the hundred different android auto models and you’re driving and you say, “Let’s have curry tonight,” we know the Warriors are on tonight and Steph Curry is playing but you know, all you’re looking for is food, and we should be smart, order that food and let you know when it is ready, and maybe even have it waiting for you at your home.
> 
> Talking about your home, we’ve already built many, many products for your home. Today, we have sold over 25 million Chromecast devices. So we’ve been thinking hard about how to bring this vision of Google Assistant into your home. Credit to the team at Amazon for creating a lot of excitement in this space, we’ve been thinking about our own unique approach and we are getting ready to launch something later this year. To give you a preview, I’m going to invite Mario from the Chromecast team. 
>
> Thanks, Erik. As you heard earlier, the Google Assistant is an ongoing dialogue between you and Google that helps you get things done in your world. It’s also designed as an ambient experience. It’s there for you whenever you need it. And in messaging that really means bringing the Google Assistant right into your conversation with friends. So I’m going to show you how the Assistant can help in Amit’s and Joy’s conversation.
> 
> So they’re planning a dinner and Joy now says she would like Italian food. The Assistant intelligently recognizes that they could use some tips for Italian restaurants nearby and you can see its proactive suggestions at the bottom of the screen there. Tapping this brings up restaurant cards that everyone in the chat can see. These are powered by Google’s Knowledge Graph which means that Allo can help with all kinds of information in the real world. So there’s some back and forth about which restaurant to go to. And it looks like they’re leaning towards Cucina at 7 o’clock.
> 
> So what we’re seeing here — what we’re seeing here is completely new. In the past, Amit would have had to leave the chat to do a Google search, return with some restaurant options, switch back again to share the options, go out again to make the reservation at OpenTable and then come back in to share the details with the rest of the group.
>
> Okay. So you just saw how the Google Assistant can be really helpful in groups. You can also have a one-on-one chat with Google. What we’re seeing now is Amit’s contact list and Google’s appearing at the top there. So let’s jump in and have a chat.
> 
> Just like with any other conversation, this one picks up right where you left off and the Assistant will remember things like your name and even tell you how it’s feeling. So let’s try something more interesting. Amit’s a big Real Madrid fan and he wants to know how they got on in their last match. So he asks the Assistant: did my team win? It looks like they did. They won their — yeah. Some Real Madrid fans out there. Cool. And so they won their last match on Saturday. Let’s see when they are playing next. That’s pretty cool. They are through the Champion’s League final at the end of the month. We can keep going like this and find more news about the team just by tapping on the suggestions there.
{% cite sGoogle2016Keynote2016 %}





**Scraps**

- **Review**: {% cite mialonAugmentedLanguageModels2023 %}
	- Also be sure to read and cite: {% cite shahSituatingSearch2022 %}
- Google integrating AI and its graph of everything: {% cite nayakMUMNewAI2021 %}
	- {% cite raghavanHowAIMaking2021 %} - this is how Lens works
	- {% cite robertsExploringTransferLearning2020 %}
- Google prior history:
	- {% cite gomesImprovingSearchNext2018 %}
	    - "neural embeddings:" from understanding words to understanding concepts and then map then into some thing in teh knowlege graph.


