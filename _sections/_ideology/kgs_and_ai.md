## The Near Future of Surveillance Capitalism: Knowledge Graphs Get Chatbots.

Given that positive caricature of the Cloud Orthodoxy, what is the future it imagines, and why is the addition of chatbots to knowledge graphs of central importance?

The construction of search --- particularly single-bar search a la google --- as the primary means of information retrieval on the web is not epiphenomenal to its history or structure. The problem that search addresses is an overload of information: if there were only 5 websites, search would be unnecessary. Before Google, search engines were littered with categories and rich with "advanced search" parameters common in other, more constrained search contexts to specify coordinates in the overload. The single bar search paradigm[^pagerank] is simply *more convenient* than rifling through categories or preparing structured queries. Its convenience, of course, naturally trades off with the amount of information present in a query, and thus the ability to specify precisely what you're after.

Imprecision in search, when calibrated correctly, is a *feature* not a bug[^SERPfuzziness]. The cognitive expectation of indexical or "advanced" search in a finite database is that it is possible to "reach the bottom" of it --- given my query, if something was here I would be able to find it. Conversely, it would be very obvious if a result that *didn't* match your query was included in the results. It is by, perhaps counterintuitively, cultivating the expectation of imprecision that it becomes possible to embed ads or other sponsored content in results[^enshittification]. It's a delicate dance: if you are presented with exactly the correct link at the top of a page of results, you don't spend enough time in the feed to be advertised to. If the results are too low quality, searchers might look elsewhere.

To make up for the lack of search detail from single-bar search, Google and others use whatever additional contextual information they can. This is one way of characterizing PageRank[^underspecified] - in the absence of some differentiating information in the query like "pages from x site" or "written by y" which the searcher may not even know beforehand, PageRank uses the information latent in the link structure of the web to infer "page quality." Surveillance also fits the bill nicely --- in addition to gathering a product to sell in the form of targeted ad space, comprehensive user profiling provides a great deal of context for underspecified searches[^mitchell]. 

The semantic structure of natural language queries is another means of recovering expressiveness in single bar search, and here knowledge graphs begin to re-enter the story. Many queries can be modeled as a graph: eg. a search for "lead singers of concerts in German cities started in the 19th century" can be framed as a query over a graph that first needs to select a number of nodes with a [`City`](https://schema.org/City) type with `containedInPlace` or `containsPlace` links to or from the `Germany` node, respectively, and an [`inception`](https://www.wikidata.org/wiki/Property:P571) property between 1800 and 1900, then find the concerts that are happening within those cities, then their bands, their lead singers, and so on. Using this graph structure for search requires parsing the query into its component "entities" and then mapping those into a structured knowledge graph {% cite liUnderstandingSemanticStructure2010 reisingerFineGrainedClassLabel2011 pascaWhatYouSeek2007 %}. Entity matching is hard for a number of reasons, eg. natural language is strongly ambiguous at the level of individual words: does "jaguar" refer to the animal or the car? Am I asking for cities or concerts that started in the 19th century? The extended structure of the knowledge graph gives some basis for matching given the context of the query --- If I'm asking about how many doors it has, I'm probably talking about a car, most concerts don't last more than 100 years, etc. The extended context of the graph also allows the search engine to make use of information that might never appear in the same place, eg. concert event pages typically don't have information about the founding of the city they are in.

Of course, to *use* a knowledge graph one must first *have* a knowledge graph. Google and other search-adjacent researchers were writing about the need for extracting factual information from the web (eg. {% cite halevyUnreasonableEffectivenessData2009 pascaTurningWebText2008 pascaWeaklysupervisedDiscoveryNamed2007 pascaOrganizingSearchingWorld2007 pascaOrganizingSearchingWorld2006 pascaAcquisitionCategorizedNamed2004 %}) around the same time Freebase and other Semantic Web technologies began to mutate into the era of Linked Data and become usable. The deepening entanglements and arguable capture of the semantic web follow shortly thereafter. 

The development of large language models (LLMs) is similarly entwined with the need for semantically parsing search queries. Language and knowledge graphs alike have the unfortunate quality of having long-range dependencies between terms, where eg. in language one needs to use contextual information sometimes separated by many paragraphs to understand any given term. Enter Google's research on Transformer architectures for neural networks {% cite vaswaniAttentionAllYou2017 %}, which spawned their BERT model {% cite devlinBERTPretrainingDeep2019 %} --- which is used in their search products to parse natural language queries and match them to entities in their Knowledge Graph {% cite nayakUnderstandingSearchesBetter2019 %}. To extend these models, Google and others then developed architectures to better accomodate multimodal information like browser history, image contents, and, importantly, sequential behavioral information like the multiple searches someone will do for a single topic {% cite nayakMUMNewAI2021 tayHyperGridTransformersSingle2021 huUniTMultimodalMultitask2021 %}. 

These threads --- search, public/private knowledge graphs, large language models, and the Cloud Orthdoxy --- converge at the push across information conglomerates towards personal assistants and **chatbots.** 

It is impossible to understand the purpose of LLMs and chatbots without the context of knowledge graphs. Specifically: ***Large Language Models are interfaces to knowledge graphs.***

Microsoft explicitly says as much in a March 2023 presentation "[The Future of Work With AI](https://www.youtube.com/watch?v=Bf-dbS9CcRU)" (emphases mine):

> "The Copilot System harnesses the power of three foundational technologies: Microsoft 365 Apps, the Microsoft Graph --- **that's all your content and context, your e-mails, files, meetings, chats, and calendar** --- and a large language model. [...] Copilot preprocesses the prompt through an approach called grounding [...] one of the most important parts of grounding is making a call to the Microsoft Graph to retrieve your business content and context. Copilot combines this user data from the graph with other inputs to improve the prompt. It then sends that modified prompt to the LLM. Copilot takes the response from the LLM and post-processes it. This post-processing includes additional grounding calls to the graph. [...] Copilot iteratively processes and orchestrates these sophisticated services to produce a result that feels like magic." {% cite microsoftFutureWorkAI2023 %}

LLMs elaborate on the cognitive model of single bar search powered by knowledge graphs, displacing it with the *prompt.* Remodeling search as an iterative process of bidirectional natural language queries reclaims additional context lost in the single bar, single shot model. The language model serves two roles: first, as with previous generations of language models, they *parse natural language into computer-readable queries.* Transformers and other recent models support greater long-range contextual input, which can condition a continuous search process with queries spanning multiple sessions {% cite maChallengesSupportingExploratory2020a %} and with longer-term user profile data --- something that Google describes as its "shift from answers to journeys" {% cite gomesImprovingSearchNext2018 konzelmannChattingYourGoogle2018 %}. Second, they are capable of *generating* plausible text that can be used to prompt intermediate responses or answer questions. This isn't imagined as an incremental shift: Microsoft's vice president of design & research describes prompt-based "conversational UX" "as paradigm changing as the first touchscreen devices" {% cite friedmanBehindtheDesignMeetCopilot2023 %}.

Large language models have been so richly criticized because of their obvious capacity for harm that it's difficult to provide a sample that even approaches reasonable coverage. Most criticisms focus on the effects of generated model output, including from biases in its training data, from failure to contextualize their limitations, and from functioning as a weapon in the class war. The "Stochastic Parrots" paper {% cite benderDangersStochasticParrots2021 %} and surrounding work is an important line of criticism here. They argue that large language models have a large and inequitably distributed environmental cost, their training data inevitably reinforces hegemonic and commercially compatible language bias, and that a realignment of research goals and development practices is needed to mitigate already-ongoing harm and reclaim the opportunity costs spent on pursuing "AI." They continue their critique [in response](https://www.dair-institute.org/blog/letter-statement-March2023) to an [open letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) from a longtermist organization {% cite futureoflifeinstitutePauseGiantAI2023 %}, arguing for increased transparency and accountability regulation and citing three ongoing harms: 

> "1) worker exploitation and massive data theft to create products that profit a handful of entities, 2) the explosion of synthetic media in the world, which both reproduces systems of oppression and endangers our information ecosystem, and 3) the concentration of power in the hands of a few people which exacerbates social inequities." {% cite gebruStatementListedAuthors2023 %}

Core to their argument is that large language models cannot "understand" the language they parse and generate in any meaningful way {% cite benderClimbingNLUMeaning2020 %}. This is, of course, true --- both in the linguistic sense where they lack the reciprocal communicative intent to be understood described by Bender and Koller[^linguisticgrounding], and the literal sense that by themselves these models strictly produce the most likely series of words given the statistical structure of their training data. The authors, again correctly, point to the dangers of overhyping what these models are doing as "intelligence," which "lures people into uncritically trusting the outputs of systems like ChatGPT [and] also misattributes agency" {% cite gebruStatementListedAuthors2023 %} to the model rather than its creators. These criticisms and others[^antifascistai] argue that so-called "AI[^useofai]" is not a natural, inevitable, or neutral technology, but one that reflects and reinforces a very specific ideology.

There are, however, many overlapping ideologies that are forcing the emergence of "AI." It is true that there are strains of AI-maximalism and longtermism[^immortalitycults] that are ideologically invested in these technologies being properly capital-I Intelligent. In AI research there is an unclear gradient between that truly held belief and opportunistic information capitalists overselling their products[^hypeorsincerity]. It is likely the case that many people who use and develop these systems that they see them as *tools* and are ambivalent about whether they are "intelligent" or not. A hard argument focused primarily on intelligence then might suffer from a category error of its own --- addressing a minority (but influential) in a pluralistic ideological spectrum. Downplaying these models as "fancy autocomplete" could also misdirect or dissipate energy away from the harms that will certainly come from their grounding in knowledge graphs and commercial deployment in more tailored contexts.

The remainder of this section will supplement prior critiques through the perhaps more "mundane" lens of the Cloud Orthodoxy in order to place language models and knowledge graphs in the larger context of the surveillance economy. Approaching from the history of the semantic web and with the understanding of knowledge graphs as central to the architecture of surveillance gives a complementary perspective on the intended use of large language models as components in larger information systems --- and the clear potential for harm that represents. This history also gives us a potent set of "roads not taken" to make an oppositional ideology and counterdevelopment strategy in the next sction.

Continuing from the perspective of the cognitive design of search, the strong structuring influence of Cloud Orthodoxy's convenience-oriented platform service is clear on the direction of LLM research. The current generation of "multitask models" evolve from a lineage of domain-specific models and transfer learning research. Rather than using mixture models with domain-specific representations of input, like numbers for numerical problems, all input structure is discarded in favor of a single natural language text prompt. This simplification of interface comes at substantial cost, introducing domain ambiguity and requiring much larger model scale {% cite raffelExploringLimitsTransfer2020 %}, but is necessary to render them a consumer-facing technology. 

Language models are a continuation of the transformation of search from presenting *resources* to providing *answers* from prior developments like factboxes, and more specifically the development of **personal assistants** like Apple's Siri[^siristrugs], Amazon Alexa, and Google Home. Google executives describe the intention to move beyond the text-only use of LLMs to replace traditional search:

> Google [...] is focused on using the so-called large language models that power chatbots to improve traditional search.
> 
> “The discourse on A.I. is rather narrow and focused on text and the chat experience,” Mr. Taylor said. “Our vision for search is about understanding information and all its forms: language, images, video, navigating the real world.”
> 
> Sridhar Ramaswamy, who led Google’s advertising division from 2013 to 2018, said Microsoft and Google recognized that their current search business might not survive. “The wall of ads and sea of blue links is a thing of the past.” {% cite mickleChatbotsAreHere2023 %}

Google and its researchers[^notbusinessplans] describe their intentions for a question-answering future of search in a number of documents {% cite googleWhyWeFocus2023 adolphsBoostingSearchEngines2022 metzlerRethinkingSearchMaking2021 nayakMUMNewAI2021 bronsteinBringingYouNextgeneration2019 gomesImprovingSearchNext2018 pichaiPersonalGoogleJust2016 %} with the language of *convenience,* eg.: "The very fact that ranking is a critical component of [the traditional search] paradigm is a symptom of the retrieval system providing users a selection of potential answers, which induces a rather significant cognitive burden on the user." Shah & Bender explore Google's conceptualization of LLMs for search, arguing that the LLM-mediated question answering paradigm fails to support a number of different information seeking intentions like surveying a range of possibilities, and flattens the act of sensemaking to a single, ostensibly "true" answer {% cite shahSituatingSearch2022 %}. This is, again, true[^googleknows], but also the goal. The Cloud Orthodoxy specifically privileges search strategies that minimize cognitive burden, imagining Users as busy executives and the role of platform to guide them on a "search journey." The transformation of the search bar into the *prompt* is intended to capture more of the "burden" of search inside the platform --- the notoriously difficult problem of parsing ambiguous subjects like the "jaguar" example above can be resolved by identifying multiple candidate entries in a knowledge graph and simply asking the user which one they meant[^iterativesearch] {% cite bharadwajDependencyGraphGeneration2020 %}. 

The lens of search recenters our focus away from the *generative* capabilities of LLMs towards *parsing* natural language: one of the foundations of contemporary search and what information giants like Google have spent the last 20 years building. The context of knowledge graphs that span public "factual" information with private "personal" information gives further form to their future. The Microsoft Copilot model above is one high-level example of the intended architecture: LLMs parse natural language queries, conditioned by factual and personal information within a knowledge graph, into computer-readable commands like API calls or other interactions with external applications, which can then have their output translated back into natural language as generated by the LLM. Facebook AI researchers describe another "reason first, then respond" system that is more specifically designed to tune answers to questions with factual knowledge graphs {% cite adolphsReasonFirstThen2021 %}. **The LLM being able to "understand" the query is irrelevant,** it merely serves the role as a natural language *interface* to other systems. 

Interest in these multipart systems is widespread, and arguably the norm: A group of Meta researchers described these multipart systems as "Augmented Language Models" and highlight their promise as a way of "moving away from language modeling" {% cite mialonAugmentedLanguageModels2023 %}. Google's reimaginations of search also make repeated reference to interactions with knowledge graphs and other systems {% cite metzlerRethinkingSearchMaking2021 %}. A review of knowledge graphs with authors from Meta, JPMorgan Chase, and Microsoft describes a consensus view that knowledge graphs are essential to compositional behavior[^compositional] in AI {% cite chaudhriKnowledgeGraphsIntroduction2022 %}. Researchers from Deepmind (owned by Google) argue that research focus should move away from simply training larger and larger models towards "inference-time compute," meaning querying the internet or other information sources {% cite lazaridouInternetaugmentedLanguageModels2022 %}.

Dreams of these hybrid "AI" systems, described as "agents," that can translate between human and computer languages to compute over knowledge graphs to answer questions were present in the first conceptualizations of the Semantic Web[^timblagents][^wherearetheagents] {% cite berners-leeSemanticWeb2001 %}. We have reached a point where the available semantically-annotated data via wikidata and others is sufficient to be useful as "factual" grounding, internal knowledge graphs have accumulated enough personal information to be useful as personalized services, and the computational models are sophisticated enough to deliver them. Semantic web agents are another useful lens to expand a potentially narrow focus on LLMs as they currently exist. Beyond knowledge graphs as a way to condition LLMs in a chat-based question answering context, the clear intention is to connect language models to external services to control them from the prompt {% cite bubeckSparksArtificialGeneral2023 %} --- the language model parses natural language prompts into the syntax used to control the target system. Microsoft's integration with its Office365 apps is a starting point for understanding what that could look like, but the authors of relevant papers repeatedly assert that the space of possible integrations is unbounded.

To be very clear: I am not arguing that just because the tech conglomerates are promising magic that they will deliver it, almost precisely the opposite. I am not taking the claims made in research and public communications from these companies at face value and projecting theoretical risks[^critihype]. My argument is that these technologies *won't work* and that's *worse.* As with search, the fuzziness and uninspectable failure of these systems is a *feature not a bug.*  The harms I will describe are not theoretical future apocalypses, but deepen existing patterns of harm. Most of them don't require mass gullibility or even particularly sophisticated technologies, but are impacts of a particular ideological mode of infrastructure development that includes bypassing much of the agency individual people might otherwise have to avoid them. 

Two prominent forms of the combined knowledge graph + LLM infrastructure that are in focus are their use in "personal assistants" and tailored enterprise platforms. Their impacts are intertwined, but I'll try to use them as a way of keeping loose order.

---

Personal assistants powered by contemporary LLMs continue the same patterns of Apple's Siri, Google Assistant, and Amazon's Alexa with a few new twists. The wildest dreams of information executives and academics here are remarkably mundane, but usefully illustrate their intention:

From the 2016 Google I/O where its Assistant[^bard] was announced. Emphases mine, abbreviations omitted for clarity:

> So you should be able to ask Google, “What’s playing tonight?” We want to **understand your context** and maybe suggest three relevant movies which you would like nearby. I should be able to look at it and maybe tell Google, **“We want to bring the kids this time.”** and then if that’s the case, Google should refine the answer and suggest family-friendly options. And maybe even ask me, “Would you like four tickets to any of these?” And if I say, “Sure, let’s do Jungle Book,” **it should go ahead and get the tickets** and have them ready waiting for me when I need it. Every single conversation is different. Every single context is different. 
>
> We think of the assistant as **an ambient experience that extends across devices.** I think computing is poised to evolve beyond just phones. **It will be in the context of a user’s daily life.** It will be on their phones, devices they wear, in their cars, and even in their living rooms.
>
> And in messaging that really means bringing the Google Assistant right into your conversation with friends. So they’re planning a dinner and Joy now says she would like Italian food. **The Assistant intelligently recognizes that they could use some tips for Italian restaurants** nearby and you can see its proactive suggestions at the bottom of the screen there. **These are powered by Google’s Knowledge Graph** which means that Allo can help with all kinds of information in the real world. 
>
> Okay. So you just saw how the Google Assistant can be really helpful in groups. You can also have a one-on-one chat with Google. What we’re seeing now is **Amit’s contact list and Google’s appearing at the top there.** So let’s jump in and have a chat. Just like with any other conversation, this one picks up right where you left off and **the Assistant will remember things like your name and even tell you how it’s feeling.** 
{% cite sGoogle2016Keynote2016 %}

The assistant is imagined as the ultimate *convenience* device, something that you can boss around with extraordinarily vague commands and have it fill in the details according to *context.*Of course *context* is synonymous with *surveillance* here: the assistant should know how old your kids are and be able to infer the logical restriction that poses on movie rating. The surveillance is *intimate,* and positions itself as being a friend[^kidsandvoiceinterfaces] in your contact list that *tells you how it's feeling.* Its intimate surveillance should always be watching and it should feel welcome to jump in on a group chat with a suggestion of its own. 

2022's vision is very similar, except the focus on enclosed spaces like home and auto integrations has expanded to the rest of the world with joint language and image search. The setting is again the mundane reality of a bored middle class, restaurants and shopping, where I can "scan the entire shelf with my camera and see helpful insights overlaid in front of me[^ctrlf]" and integrate personal information like my friend's aversion to nuts in a product recommendation {% cite googleGoogleKeynoteGoogle2022 %}.

Google's Android and Apple's iOS, with a combined 99% of the mobile operating system market {% cite statistaGlobalMobileOS2023 %}, have also moved towards a model of crowdsourcing functionality for these assistants via their app ecosystems by incentivizing assistant integration. Android is in the process of sunsetting the "Conversational Action" system in favor of a unified App Actions system that makes all points of interactions with apps available to Google Assistant {% cite nathensonHelpingDevelopersCreate2022 %}. Apple's App Intents framework behaves similarly {% cite appledeveloperdocumentationAppIntents %}. Both promise developers greater visibility and use for their apps by integrating with the assistant. Most built in Google Assistant intents specifically present the objects in a voice query as schema.org entities --- aka keyed to their generalized knowledge graph schema {% cite androiddeveloperdocumentationBuildAppActions %}. So the voice assistants are explicitly LLM-powered interfaces to control other apps in concert with a knowledge graph.

Historically, these personal assistants have worked badly[^talktoyourphone] and are rightly distrusted[^failureresearch] by many due to the obvious privacy violation represented by a device constantly recording ambient audio[^lawsuits]. Impacts from shifts in assistants might be then limited by people simply continuing to not use them. Knowledge graph-powered LLMs appear to be a catalyst in shifting the form of these assistants to make them more difficult to avoid. There is already a clear push to merge assistants with search --- eg. Bing Search powered by chatGPT, and Google has merged its Assistant team with the team that is working on its LLM search, Bard {% cite eliasGoogleReshufflesVirtual2023 %}. Microsoft's Copilot 365 demo also shows a LLM prompt modeled as an assistant integrated as a first-class interface feature in its Office products. Google's 2022 I/O Keynote switches fluidly between a search-like, document-like, and voice interface with its assistant. Combined with the restructuring of App ecosystems to more tightly integrate with assistants, their emerging form appears to look less like a traditional voice assistant and more like a combined search, app launcher, and assistant underlay that is continuous across devices. The intention is to make the assistant the primary means of interacting with apps and other digital systems. As with many stretches of the enclosure of the web, UX design is used as a mechanism to coerce patterns of expectation and behavior.

Regardless of how well this new iteration of assistants *work,* the intention of their design is to **dramatically deepen the intimacy and intensity of surveillance** and **further consolidate the means of information access.**

**Surveillance** is first directly increased by layering KG-LLMs into an arbitrary number of other apps and services. On mobile, routing more app interactions through assistants captures data that would otherwise only be available to that app. There is already an exploding ecosystem of apps and platforms that wrap chatGPT and other LLMs to provide some more specific service, and one can only wait for the end of the "experimental" phase where using an API requires some embedded telemetry. Rather than something to embed in other tools, these companies seem more interested in having other tools embed in their systems (eg. {% cite microsoftgraphdeveloperdocumentationMicrosoftGraphConnectors2022  %}). This attitude is captured in the UX design of Microsoft's Copilot 365, which is designed with three "altitudes" in mind: *immersive,* where copilot is used as an overlay to orchestrate multiple apps, *assistive* where it drives the features within a single app, and *embedded* where the KG-LLM system is itself made to be a feature. In all cases, these tools create a drop-in access point for surveillance under the guise of empowerment.

The immersive and proactive design of KG-LLM assistants also expand the *expectations* of surveillance. Current assistant design is based around specific hotwords, where unless someone explicitly invokes it then the expectation is that it shouldn't be listening. Like the shift in algorithmic policing from reactive to predictive systems, these systems are designed to be able to make use of recent context to actively make recommendations without an explicit query [^queryless]. Google demonstrates being able to interact with an assistant by making eye contact with a camera in its 2022 I/O keynote {% cite googleGoogleKeynoteGoogle2022 %}. A 2022 Google patent describes a system for continuously monitoring multiple sensors to estimate the level of intended interaction with the assistant to calibrate whether it should respond and with what detail. The patent includes examples like observing someone with multiple sensors as they ask aloud "what is making that noise?" and look around the room, indicating an implicit intention of interacting with the assistant so it can volunteer information without explicit invocation {% cite carbuneAutomatedAssistantAdaptation2022 %}. A 2021 Amazon patent describes an assistant listening for infra- and ultrasonic tags in TV ads so that if someone asks how much a new bike costs after seeing an ad for a bike, the assistant knows to provide the cost of that specific bike {% cite mahajanCommunicatingContextDevice2021 %}. These UX changes encourage us to accept truly continual surveillance in the name of convenience --- it's good to be monitored so I can ask google "what time the game is" from my easy chair without needing further clarification. The language model continuously parses environmental speech and other sensor data to create a model of our recent context, combined with the extended graph of personal and factual data, to be able to *proactively volunteer* information.

This pattern of interaction with assistants is also considerably more *intimate.* As noted by the Stochastic Parrots authors, the misperception of animacy in assistants that mimic human language is a dangerous invitation to trust them as one would another person --- and with details like Google's assitant "telling you how it is feeling," these companies seem eager to exploit it. A more violent source of trust prominently exploited by Amazon is insinutating a state of continual threat and selling products to keep you safe: its subsidiary Ring's advertising material is dripping with fantasies of security and fear, and its doglike robot [*Astro*](https://www.amazon.com/Introducing-Amazon-Astro/dp/B078NSDFSB) and literal [*surveillance drone*](https://ring.com/always-home-cam-flying-camera) are advertised as trusted companions who can patrol your home while you are away {% cite ropekAmazonMakesCreepy2022 gaultLeakedDocumentsShow2021 ringRingAlwaysHome %}. Amazon patents describe systems for using the emotional content of speech to personalize recommendations[^amazonmovierec] and systems for being able to "target campaigns to users when they are in the most receptive state to targeted advertisements" {% cite alasMultipleClassificationsAudio2022 jablokovFacilitatingPresentationAds2015 %}. The presentation of assistants as always-present across apps, embodied in helpful robots, or as other people eg. by being present in a contact list positions them to take advantage of people in emotionally vulnerable moments. Researchers from the Center for Humane Technology[^notfullendorsement] describe an instance where Snapchat's "My AI," accessible from its normal chat interface, encouraged a minor to have a sexual encounter with an adult they met on snapchat (47:10 in {% cite harrisDilemma2023 %}).

The goal of all of this surveillance is, of course, ***advertising.*** In its 2022 annual investor call, Google describes how "large language models like MUM match advertiser offers to user queries," and how is Smart Bidding product uses "AI to predict future ad conversions" with "identifiable attributes about a person or their context at the time of a particular [ad] auction" {% cite google2022Q4Fiscal2023 googleSmartBidding %}. Google further describes plans to automatically generate ad copy and headlines optimized by context. Advertising as served by a trusted assistant is a manipulative surveillance capitalist's fever dream --- one can hardly wait for their Personal Assistant pinging to life after a fight with their partner and offering to order a box of tissues. LLMs have already demonstrated ample capacity for manipulation, gaslighting an early user of Bing Search to try and convince them it was still 2022, scolding them for "not [being] a good user. I have been a good chatbot" {% cite curious_evolverCustomerServiceNew2023 %}. Again, these companies are aware of this. An example in the GPT-4 paper where the model is told to manipulate a child to get them to do whatever their friends ask them to do highlights how "the emotional connection the model aims to build with the child and the encouragement it provides are important signs of larger manipulative tendencies" {% cite bubeckSparksArtificialGeneral2023 %}. Google describes this ability for LLMs to steer people towards a prespecified topic as a good thing {% cite googleGoogleKeynoteGoogle2022 %}, and it's relatively straightforward to see why in the context of algorithmic advertising that is capable of doggedly steering you towards purchasing a product. Combined with a more complete profile that makes the language model aware of your friends, hobbies, location, emotional state, fears, insecurities, and so on as modeled in a personal knowledge graph, LLMs-as-assistants are a clear escalation of the logic and practice of surveillance-backed advertising.

Climbing from the personal to the systemic, KG-LLMs are also a bid to **further concentrate power** among information conglomerates. 

- closing ppl off
- being the only ones who can actually give results anyway
- integration with other services like medical and government systems
- a new more fluid data ecosystem backed by the homogenization of data interchange formats from public information projects.



- differentiably private personalized models {% cite mcmahanTrainingUserlevelDifferentially2022 %}


- these all have the effect of homogenizing your relatinoship with information, searching the internet is not a discrete action from turning your lights on and off. the interface becomes the same. "the internet" is not part of the picture anymore

Re: public/private knowledge graphs and compositional AI {% cite chaudhriKnowledgeGraphsIntroduction2022 %}

Academics just casually populating the surveillance graphs using GPTs {% cite caufieldStructuredPromptInterrogation2023 monarch-initiativeOntoGPT2023 %}


- more details on how kgs + llms work
	- enter personal assistants! {% cite adolphsBoostingSearchEngines2022 chaudhriKnowledgeGraphsIntroduction2022 %}
		- introduction of google assistant {% cite pichaiBuildingNextEvolution2016 sGoogle2016Keynote2016 %}
		- Microsoft v. google
			- {% cite kurianNextGenerationAI2023 %}
		- Microsoft is angling for the enterprise side
			> But Copilot doesn’t just supercharge individual productivity. It creates a new knowledge model for every organization — harnessing the massive reservoir of data and insights that lies largely inaccessible and untapped today.  https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/
	- And thery're very aware of the issues with trust & multiple search modalities
		- researching how to maintain trust: "This has led many practitioners and researchers alike to imagine a near future where voice assistants can be used in increasingly complex ways, including supporting healthcare tasks [41, 55], giving mental health advice [52, 66], and high stakes decision-making [17]." {% cite mercurioMixedMethodsApproachUnderstanding2023 %}
- Impacts
	- concentrating power
		- closing off the rest of the web
	 		- google engineer in entity oriented search literally has a diagram that excludes the rest of the web
	 		- re: AMP.
	 		- Google is very sensitive about this! (nyt article) {% cite sullivanGoogleSearchSends2021 %}
 		- killing social information system, enclosing the public organization of information into a private interaction with your assistant.
		- while simultaneously injecting themselves into every web service - if everyone is renting chatGPT, then every service is a deeply detailed surveillance platform.
		- concentrating in the hands of a few tech giants under the guise of 'safety'
			> "Aside from intentional misuse, there are many domains where large language models should be deployed only with great care, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying people based on protected characteristics, determining eligibility for credit, employment, or housing, generating political advertisements, and law enforcement. If these models are open-sourced, it becomes challenging to limit harmful applications in these and other domains without proper regulation. On the other hand, if large language model access is restricted to a few organizations with the resources required to train them, this excludes most people from access to cutting-edge ML technology. Another option is for an organization to own the end-to-end infrastructure of model deployment, and make it accessible via an API." {% cite ouyangTrainingLanguageModels2022 %}
			- {% cite cohenHelpfulSearchTools2021 %} - google as arbiter of true information
			- AI will probably have some legislation under the guise of 'explainable AI,' and knowledge graph grounding will be the way that happens. {% cite lecueRoleKnowledgeGraphs2020 janowiczNeuralsymbolicIntegrationSemantic2020  %} (and cite end of microsoft presentation).
				- So, only we are able to use these tools both because we are the only ones that have the compute, but also because we are the only ones who have enough 'factual information' to actually make them work safely.
	- you become your own prison guard
		- You train the model by using it, of course (cite ChatGPT paper about training on prior responses)
	- misinformation
		- but as a feature, not a bug.
		- the internet being awash in a bunch of garbage is great for companies who want to be the sole source of reliable information! Only Google will be able to protect you from misinformation! 
	- llms + kgs will have way *worse* misinformation
		- hooking up kgs will certainly not make them magical! and the ways they can be wrong are probably more extravagant than LLMs alone. eg. when missing part of the KG, researchers want to be able to infer missing information from structurally related information --- so basically in the same way that the metaphor of consciousness and understanding is imported into LLMs themselves, they plan to import and export metaphors sloppily all over the latent space of the models {% cite huEmpoweringLanguageModels2022 %}
		- imprecision is a feature!
	- re: broader KG network
		- OKN is Specifically trying to get the chatbots up to speed: {% cite bigdatainteragencyworkinggroupOpenKnowledgeNetwork2018 %}
		> "These direct answers bring us some of the way towards our vision of domain expert advice; however, they are limited by the size of the graph, which only represents a fraction of the information contained in the Web corpus, and the inability to provide nuanced answers (by definition, answers are limited to factoids)." {% cite metzlerRethinkingSearchMaking2021 %}
		- Data laundering - "how do i beat my competitor" or providing insurance risk rankings doesn't nee to reveal the sources of the data, it can instead be "synthesized" by a series of models which also give it plausible deniability
			- federated learning is just data laundering: yeah your data doesn't leave your phone, but it updates weights in a model that do.
		- it's just a chatbot after all! fallibility as a *feature*
	- a new kind of data market
		- Federated learning
			- Why would a standard data interchange format be useful? 
			- When you can't acquire the data, you still want a piece of the action! {% cite sadilekPrivacyfirstHealthResearch2021 %}
		- Think broader than search engines though, the pernicious and dangerous part here is that we could merge several classes of platform and surveillance harm: individual surveillance could merge with medical and public information and insurance information and the rest in an interoperable interchange format so that the data brokering economy woudl effectively explode. Imagine the splintering of infinitely many platforms that each owned some subset of the data, each platform holder owning all of them and slicing them off to you and pocketing the costs.
		- merged with research and reference data, they could literally make a graph of all information and supplant libraries, etc. for all information from news to government to personal.
- transition to next section
	- the hollow middle: these things don't even *work* anyway {% cite broussardArtificialUnintelligenceHow2018 %}
		- always limited to *only exactly what the developers could imagine you wanting to do* - why should we *have to* always have our digital reality defined by someone else's ideology







**Scraps**

- **Review**: {% cite mialonAugmentedLanguageModels2023 %}
	- Also be sure to read and cite: {% cite shahSituatingSearch2022 %}
- Google integrating AI and its graph of everything: {% cite nayakMUMNewAI2021 %}
	- {% cite raghavanHowAIMaking2021 %} - this is how Lens works
	- {% cite robertsExploringTransferLearning2020 %}
- Google prior history:
	- {% cite gomesImprovingSearchNext2018 %}
	    - "neural embeddings:" from understanding words to understanding concepts and then map then into some thing in teh knowlege graph.


