## The Near Future of Surveillance Capitalism: Knowledge Graphs Get Chatbots.

Given that positive caricature of the Cloud Orthodoxy, what is the future it imagines, and why is the addition of chatbots to knowledge graphs of central importance?

The construction of search --- particularly single-bar search a la google --- as the primary means of information retrieval on the web is not epiphenomenal to its history or structure. The problem that search addresses is an overload of information: if there were only 5 websites, search would be unnecessary. Before Google, search engines were littered with categories and rich with "advanced search" parameters common in other, more constrained search contexts to specify coordinates in the overload. The single bar search paradigm[^pagerank] is simply *more convenient* than rifling through categories or preparing structured queries. Its convenience, of course, naturally trades off with the amount of information present in a query, and thus the ability to specify precisely what you're after.

Imprecision in search, when calibrated correctly, is a *feature* not a bug[^SERPfuzziness]. The cognitive expectation of indexical or "advanced" search in a finite database is that it is possible to "reach the bottom" of it --- given my query, if something was here I would be able to find it. Conversely, it would be very obvious if a result that *didn't* match your query was included in the results. It is by, perhaps counterintuitively, cultivating the expectation of imprecision that it becomes possible to embed ads or other sponsored content in results[^enshittification]. It's a delicate dance: if you are presented with exactly the correct link at the top of a page of results, you don't spend enough time in the feed to be advertised to. If the results are too low quality, searchers might look elsewhere.

To make up for the lack of search detail from single-bar search, Google and others use whatever additional contextual information they can. This is one way of characterizing PageRank[^underspecified] - in the absence of some differentiating information in the query like "pages from x site" or "written by y" which the searcher may not even know beforehand, PageRank uses the information latent in the link structure of the web to infer "page quality." Surveillance also fits the bill nicely --- in addition to generating a product to sell in the form of targeted ad space, comprehensive user profiling provides a great deal of context for underspecified searches[^mitchell]. 

The semantic structure of natural language queries is another means of recovering expressiveness in single bar search, and here knowledge graphs begin to re-enter the story. Many queries can be modeled as a graph: eg. a search for "lead singers of concerts in German cities started in the 19th century" can be framed as a query over a graph that first needs to select a number of nodes with a [`City`](https://schema.org/City) type with `containedInPlace` or `containsPlace` links to or from the `Germany` node, respectively, and an [`inception`](https://www.wikidata.org/wiki/Property:P571) property between 1800 and 1900, then find the concerts that are happening within those cities, then their bands, their lead singers, and so on. Using this graph structure for search requires parsing the query into its component "entities" and then mapping those into a structured knowledge graph {% cite liUnderstandingSemanticStructure2010 reisingerFineGrainedClassLabel2011 pascaWhatYouSeek2007 %}. Entity matching is hard for a number of reasons, eg. natural language is strongly ambiguous at the level of individual words: does "jaguar" refer to the animal or the car? Am I asking for cities or concerts that started in the 19th century? The extended structure of the knowledge graph gives some basis for matching given the context of the query --- If I'm asking about how many doors it has, I'm probably talking about a car, most concerts don't last more than 100 years, etc. The extended context of the graph also allows the search engine to make use of information that might never appear in the same place, eg. concert event pages typically don't have information about the founding of the city they are in.

Of course, to *use* a knowledge graph one must first *have* a knowledge graph. Google and other search-adjacent researchers were writing about the need for extracting factual information from the web (eg. {% cite halevyUnreasonableEffectivenessData2009 pascaTurningWebText2008 pascaWeaklysupervisedDiscoveryNamed2007 pascaOrganizingSearchingWorld2007 pascaOrganizingSearchingWorld2006 pascaAcquisitionCategorizedNamed2004 %}) around the same time Freebase and other Semantic Web technologies began to mutate into the era of Linked Data and become usable. The deepening entanglements and arguable capture of the semantic web follow shortly thereafter. 

The development of large language models (LLMs) is similarly entwined with the need for semantically parsing search queries. Language and knowledge graphs alike have the unfortunate quality of having long-range dependencies between terms, where eg. in language one needs to use contextual information sometimes separated by many paragraphs to understand any given term. Enter Google's research on Transformer architectures for neural networks {% cite vaswaniAttentionAllYou2017 %}, which spawned their BERT model {% cite devlinBERTPretrainingDeep2019 %} --- which is used in their search products to parse natural language queries and match them to entities in their Knowledge Graph {% cite nayakUnderstandingSearchesBetter2019 %}. To extend these models, Google and others then developed architectures to better accomodate multimodal information like browser history, image contents, and, importantly, sequential behavioral information like the multiple searches someone will do for a single topic {% cite nayakMUMNewAI2021 tayHyperGridTransformersSingle2021 huUniTMultimodalMultitask2021 %}. 

These threads --- search, public/private knowledge graphs, large language models, and the Cloud Orthdoxy --- converge at the push across information conglomerates towards personal assistants and **chatbots.** 

It is impossible to understand the purpose of LLMs and chatbots without the context of knowledge graphs. Specifically: ***Large Language Models are interfaces to knowledge graphs.***

Microsoft explicitly says as much in a March 2023 presentation "[The Future of Work With AI](https://www.youtube.com/watch?v=Bf-dbS9CcRU)" (emphases mine):

> "The Copilot System harnesses the power of three foundational technologies: Microsoft 365 Apps, the Microsoft Graph --- **that's all your content and context, your e-mails, files, meetings, chats, and calendar** --- and a large language model. [...] Copilot preprocesses the prompt through an approach called grounding [...] one of the most important parts of grounding is making a call to the Microsoft Graph to retrieve your business content and context. Copilot combines this user data from the graph with other inputs to improve the prompt. It then sends that modified prompt to the LLM. Copilot takes the response from the LLM and post-processes it. This post-processing includes additional grounding calls to the graph. [...] Copilot iteratively processes and orchestrates these sophisticated services to produce a result that feels like magic." {% cite microsoftFutureWorkAI2023 %}

LLMs elaborate on the cognitive model of single bar search powered by knowledge graphs, displacing it with the *prompt.* Remodeling search as an iterative process of bidirectional natural language queries reclaims additional context lost in the single bar, single shot model. The language model serves two roles: first, as with previous generations of language models, they *parse natural language into computer-readable queries.* Transformers and other recent models support greater long-range contextual input, which can condition a continuous search process with queries spanning multiple sessions {% cite maChallengesSupportingExploratory2020a %} and with longer-term user profile data --- something that Google describes as its "shift from answers to journeys" {% cite gomesImprovingSearchNext2018 konzelmannChattingYourGoogle2018 %}. Second, they are capable of *generating* plausible text that can be used to prompt intermediate responses or answer questions. This isn't imagined as an incremental shift: Microsoft's vice president of design & research describes prompt-based "conversational UX" "as paradigm changing as the first touchscreen devices" {% cite friedmanBehindtheDesignMeetCopilot2023 %}.

Large language models have been so richly criticized because of their obvious capacity for harm that it's difficult to provide a sample that approaches reasonable coverage. Most criticisms focus on the effects of generated model output, including from biases in its training data, from failure to contextualize their limitations, and from functioning as a weapon in the class war by automating labor. The "Stochastic Parrots" paper {% cite benderDangersStochasticParrots2021 %} and surrounding work is an important line of criticism here. The authors argue that large language models have a large and inequitably distributed environmental cost, their training data inevitably reinforces hegemonic and commercially compatible language bias, and that a realignment of research goals and development practices is needed to mitigate already-ongoing harm and reclaim the opportunity costs spent on pursuing "AI." They continue their critique [in response](https://www.dair-institute.org/blog/letter-statement-March2023) to an [open letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) from a longtermist organization {% cite futureoflifeinstitutePauseGiantAI2023 %}, arguing for increased transparency and accountability regulation and citing three ongoing harms: 

> "1) worker exploitation and massive data theft to create products that profit a handful of entities, 2) the explosion of synthetic media in the world, which both reproduces systems of oppression and endangers our information ecosystem, and 3) the concentration of power in the hands of a few people which exacerbates social inequities." {% cite gebruStatementListedAuthors2023 %}

Core to their argument is that large language models cannot "understand" the language they parse and generate in any meaningful way {% cite benderClimbingNLUMeaning2020 %}. This is, of course, true --- both in the linguistic sense where they lack the reciprocal communicative intent to be understood described by Bender and Koller[^linguisticgrounding], and the literal sense that by themselves these models strictly produce the most likely series of words given the statistical structure of their training data. The authors, again correctly, point to the dangers of overhyping what these models are doing as "intelligence," which "lures people into uncritically trusting the outputs of systems like ChatGPT [and] also misattributes agency" {% cite gebruStatementListedAuthors2023 %} to the model rather than its creators. These criticisms and others[^antifascistai] argue that so-called "AI[^useofai]" is not a natural, inevitable, or neutral technology, but one that reflects and reinforces a very specific ideology.

There are, however, many overlapping ideologies that are forcing the emergence of "AI." It is true that there are strains of AI-maximalism and longtermism[^immortalitycults] that are ideologically invested in these technologies being properly capital-I Intelligent. In AI research there is an unclear gradient between that truly held belief and opportunistic information capitalists overselling their products[^hypeorsincerity]. It is likely the case that many people who use and develop these systems see them as *tools* and are ambivalent about whether they are "intelligent" or not. A hard argument focused primarily on intelligence then might suffer from a category error of its own --- addressing a minority (but influential) in a pluralistic ideological spectrum. Downplaying these models as "fancy autocomplete" could also misdirect or dissipate energy away from the harms that will certainly come from their grounding in knowledge graphs and commercial deployment in more tailored contexts.

The remainder of this section will extend these prior critiques through the lens of the Cloud Orthodoxy in order to place language models and knowledge graphs in the larger context of the surveillance economy. Approaching from the history of the semantic web and with the understanding of knowledge graphs as central to the architecture of surveillance gives a complementary perspective on the intended use of large language models as components in larger information systems --- and the clear potential for harm that represents. This history also gives us a potent set of "roads not taken" to make an oppositional ideology and counterdevelopment strategy in the next sction.

Continuing from the perspective of the cognitive design of search, the strong structuring influence of Cloud Orthodoxy's convenience-oriented platform service is clear on the direction of LLM research. The current generation of "multitask models" evolve from a lineage of domain-specific models and transfer learning research. Rather than using mixture models with domain-specific representations of input, like numbers for numerical problems, all input structure is discarded in favor of a single natural language text prompt. This simplification of interface comes at substantial cost, introducing domain ambiguity and requiring much larger model scale {% cite raffelExploringLimitsTransfer2020 %}, but is necessary to render them a consumer-facing technology. 

Language models are a continuation of the transformation of search from presenting *resources* to providing *answers* from prior developments like factboxes, and more specifically the development of **personal assistants** like Apple's Siri[^siristrugs], Amazon Alexa, and Google Home. Google executives describe the intention to move beyond the text-only use of LLMs to replace traditional search:

> Google [...] is focused on using the so-called large language models that power chatbots to improve traditional search.
> 
> “The discourse on A.I. is rather narrow and focused on text and the chat experience,” Mr. Taylor said. “Our vision for search is about understanding information and all its forms: language, images, video, navigating the real world.”
> 
> Sridhar Ramaswamy, who led Google’s advertising division from 2013 to 2018, said Microsoft and Google recognized that their current search business might not survive. “The wall of ads and sea of blue links is a thing of the past.” {% cite mickleChatbotsAreHere2023 %}

Google and its researchers[^notbusinessplans] describe their intentions for a question-answering future of search in a number of documents {% cite googleWhyWeFocus2023 adolphsBoostingSearchEngines2022 metzlerRethinkingSearchMaking2021 nayakMUMNewAI2021 bronsteinBringingYouNextgeneration2019 gomesImprovingSearchNext2018 pichaiPersonalGoogleJust2016 %} with the language of *convenience,* eg.: "The very fact that ranking is a critical component of [the traditional search] paradigm is a symptom of the retrieval system providing users a selection of potential answers, which induces a rather significant cognitive burden on the user." Shah & Bender explore Google's conceptualization of LLMs for search, arguing that the LLM-mediated question answering paradigm fails to support a number of different information seeking intentions like surveying a range of possibilities, and flattens the act of sensemaking to a single, ostensibly "true" answer {% cite shahSituatingSearch2022 %}. This is, again, true[^googleknows], and also the goal. The Cloud Orthodoxy specifically privileges search strategies that minimize cognitive burden, imagining Users as busy executives and the role of platform to guide them on a "search journey." The transformation of the search bar into the *prompt* is intended to capture more of the "burden" of search inside the platform --- the notoriously difficult problem of parsing ambiguous subjects like the "jaguar" example above can be resolved by identifying multiple candidate entries in a knowledge graph and simply asking the user which one they meant[^iterativesearch] {% cite bharadwajDependencyGraphGeneration2020 %}. The platform serves as a medium for collecting feedback and refining the models, making them more useful, and deepening reliance on them.

The lens of search recenters our focus away from the *generative* capabilities of LLMs towards *parsing* natural language: one of the foundations of contemporary search and what information giants like Google have spent the last 20 years building. The context of knowledge graphs that span public "factual" information with private "personal" information gives further form to their future. The Microsoft Copilot model above is one high-level example of the intended architecture: LLMs parse natural language queries, conditioned by factual and personal information within a knowledge graph, into computer-readable commands like API calls or other interactions with external applications, which can then have their output translated back into natural language as generated by the LLM. Facebook AI researchers describe another "reason first, then respond" system that is more specifically designed to tune answers to questions with factual knowledge graphs {% cite adolphsReasonFirstThen2021 %}. **The LLM being able to "understand" the query is irrelevant,** it merely serves the role as a natural language *interface* to other systems. 

Interest in these multipart systems is widespread, and arguably the norm: A group of Meta researchers described these multipart systems as "Augmented Language Models" and highlight their promise as a way of "moving away from language modeling" {% cite mialonAugmentedLanguageModels2023 %}. Google's reimaginations of search also make repeated reference to interactions with knowledge graphs and other systems {% cite metzlerRethinkingSearchMaking2021 %}. A review of knowledge graphs with authors from Meta, JPMorgan Chase, and Microsoft describes a consensus view that knowledge graphs are essential to compositional behavior[^compositional] in AI {% cite chaudhriKnowledgeGraphsIntroduction2022 %}. Researchers from Deepmind (owned by Google) argue that research focus should move away from simply training larger and larger models towards "inference-time compute," meaning querying the internet or other information sources {% cite lazaridouInternetaugmentedLanguageModels2022 %}.

Dreams of these hybrid "AI" systems, described as "agents," that can translate between human and computer languages to compute over knowledge graphs to answer questions were present in the first conceptualizations of the Semantic Web[^timblagents][^wherearetheagents] {% cite berners-leeSemanticWeb2001 %}. We have reached a point where the available semantically-annotated data via wikidata and others is sufficient to be useful as "factual" grounding, internal knowledge graphs have accumulated enough personal information to be useful as personalized services, and the computational models are sophisticated enough to deliver them. Semantic web agents are another useful lens to expand a potentially narrow focus on LLMs as they currently exist. Beyond knowledge graphs as a way to condition LLMs in a chat-based question answering context, the clear intention is to connect language models to external services to control them from the prompt {% cite bubeckSparksArtificialGeneral2023 %} --- the language model parses natural language prompts into the syntax used to control the target system. Microsoft's integration with its Office365 apps is a starting point for understanding what that could look like, but the authors of relevant papers repeatedly assert that the space of possible integrations is unbounded.

To be very clear: I am not arguing that just because the tech conglomerates are promising magic that they will deliver it, almost precisely the opposite. I am not taking the claims made in research and public communications from these companies at face value and projecting theoretical risks[^critihype]. My argument is that these technologies *won't work* and that's *worse.* As with search, the fuzziness and uninspectable failure of these systems is a *feature not a bug.*  The harms I will describe are not theoretical future apocalypses, but deepen existing patterns of harm. Most of them don't require mass gullibility or even particularly sophisticated technologies, but are impacts of a particular ideological mode of infrastructure development that includes bypassing much of the agency individual people might otherwise have to avoid them. 

Two prominent forms of the combined knowledge graph + LLM infrastructure that are in focus are their use in "personal assistants" and tailored enterprise platforms.

---

Personal assistants powered by contemporary LLMs continue the same patterns of Apple's Siri, Google Assistant, and Amazon's Alexa with a few new twists. The wildest dreams of information executives and academics here are remarkably mundane, but usefully illustrate their intention:

From the 2016 Google I/O where its Assistant[^bard] was announced. Emphases mine, abbreviations omitted for clarity:

> So you should be able to ask Google, “What’s playing tonight?” We want to **understand your context** and maybe suggest three relevant movies which you would like nearby. I should be able to look at it and maybe tell Google, **“We want to bring the kids this time.”** and then if that’s the case, Google should refine the answer and suggest family-friendly options. And maybe even ask me, “Would you like four tickets to any of these?” And if I say, “Sure, let’s do Jungle Book,” **it should go ahead and get the tickets** and have them ready waiting for me when I need it. Every single conversation is different. Every single context is different. 
>
> We think of the assistant as **an ambient experience that extends across devices.** I think computing is poised to evolve beyond just phones. **It will be in the context of a user’s daily life.** It will be on their phones, devices they wear, in their cars, and even in their living rooms.
>
> And in messaging that really means bringing the Google Assistant right into your conversation with friends. So they’re planning a dinner and Joy now says she would like Italian food. **The Assistant intelligently recognizes that they could use some tips for Italian restaurants** nearby and you can see its proactive suggestions at the bottom of the screen there. **These are powered by Google’s Knowledge Graph** which means that Allo can help with all kinds of information in the real world. 
>
> Okay. So you just saw how the Google Assistant can be really helpful in groups. You can also have a one-on-one chat with Google. What we’re seeing now is **Amit’s contact list and Google’s appearing at the top there.** So let’s jump in and have a chat. Just like with any other conversation, this one picks up right where you left off and **the Assistant will remember things like your name and even tell you how it’s feeling.** 
{% cite sGoogle2016Keynote2016 %}

The assistant is imagined as the ultimate *convenience* device, something that you can boss around with extraordinarily vague commands and have it fill in the details according to *context.* Of course *context* is synonymous with *surveillance* here: the assistant should know how old your kids are and be able to infer the logical restriction that poses on movie rating. The surveillance is *intimate,* and positions itself as being a friend[^kidsandvoiceinterfaces] in your contact list that *tells you how it's feeling.* Its intimate surveillance should always be watching and it should feel welcome to jump in on a group chat with a suggestion of its own. 

2022's vision is very similar, except the focus on enclosed spaces like home and auto integrations has expanded to the rest of the world with joint language and image search. The setting is again the mundane reality of a bored middle class, restaurants and shopping, where I can "scan the entire shelf with my camera and see helpful insights overlaid in front of me[^ctrlf]" and integrate personal information like my friend's aversion to nuts in a product recommendation {% cite googleGoogleKeynoteGoogle2022 %}.

Google's Android and Apple's iOS, with a combined 99% of the mobile operating system market {% cite statistaGlobalMobileOS2023 %}, have adopted a model of crowdsourcing functionality for these assistants via their app ecosystems by incentivizing assistant integration[^decentralizedplatforms]. Android is in the process of sunsetting the "Conversational Action" system in favor of a unified App Actions system that makes all points of interactions with apps available to Google Assistant {% cite nathensonHelpingDevelopersCreate2022 %}. Apple's App Intents framework behaves similarly {% cite appledeveloperdocumentationAppIntents %}. Both promise developers greater visibility and use for their apps by integrating with the assistant. Most built in Google Assistant intents specifically present the objects in a voice query as schema.org entities --- aka keyed to their generalized knowledge graph schema {% cite androiddeveloperdocumentationBuildAppActions %}. So the voice assistants are explicitly LLM-powered interfaces to control other apps in concert with a knowledge graph.

Historically, these personal assistants have worked badly[^talktoyourphone] and are rightly distrusted[^failureresearch] by many due to the obvious privacy violation represented by a device constantly recording ambient audio[^lawsuits]. Impacts from shifts in assistants might be then limited by people simply continuing to not use them. Knowledge graph-powered LLMs appear to be a catalyst in shifting the form of these assistants to make them more difficult to avoid. There is already a clear push to merge assistants with search --- eg. Bing Search powered by chatGPT, and Google has merged its Assistant team with the team that is working on its LLM search, Bard {% cite eliasGoogleReshufflesVirtual2023 %}. Microsoft's Copilot 365 demo also shows a LLM prompt modeled as an assistant integrated as a first-class interface feature in its Office products. Google's 2022 I/O Keynote switches fluidly between a search-like, document-like, and voice interface with its assistant. Combined with the restructuring of App ecosystems to more tightly integrate with assistants, their emerging form appears to look less like a traditional voice assistant and more like a combined search, app launcher, and assistant underlay that is continuous across devices. The intention is to make the assistant the primary means of interacting with apps and other digital systems. As with many stretches of the enclosure of the web, UX design is used as a mechanism to coerce patterns of expectation and behavior.

Regardless of how well this new iteration of assistants *work,* the intention of their design is to **dramatically deepen the intimacy and intensity of surveillance** and **further consolidate the means of information access.**

**Surveillance** is first directly increased by layering KG-LLMs into an arbitrary number of other apps and services. On mobile, routing more app interactions through assistants captures data that would otherwise only be available to that app. There is already an exploding ecosystem of apps and platforms that wrap chatGPT and other LLMs to provide some more specific service, and it's unclear if after an initial "experimental" phase if platform usage will begin to require telemetry. Rather than something to embed in other tools, these companies seem more interested in having other tools embed in their systems (eg. {% cite microsoftgraphdeveloperdocumentationMicrosoftGraphConnectors2022  %}). This attitude is captured in the UX design of Microsoft's Copilot 365, which is designed with three "altitudes" in mind: *immersive,* where copilot is used as an overlay to orchestrate multiple apps, *assistive* where it drives the features within a single app, and *embedded* where the KG-LLM system is itself made to be a feature. In all cases, these tools create a drop-in access point for surveillance under the guise of empowerment.

The immersive and proactive design of KG-LLM assistants also expand the *expectations* of surveillance. Current assistant design is based around specific hotwords, where unless someone explicitly invokes it then the expectation is that it shouldn't be listening. Like the shift in algorithmic policing from reactive to predictive systems, these systems are designed to be able to make use of recent context to actively make recommendations without an explicit query [^queryless]. Google demonstrates being able to interact with an assistant by making eye contact with a camera in its 2022 I/O keynote {% cite googleGoogleKeynoteGoogle2022 %}. A 2022 Google patent describes a system for continuously monitoring multiple sensors to estimate the level of intended interaction with the assistant to calibrate whether it should respond and with what detail. The patent includes examples like observing someone with multiple sensors as they ask aloud "what is making that noise?" and look around the room, indicating an implicit intention of interacting with the assistant so it can volunteer information without explicit invocation {% cite carbuneAutomatedAssistantAdaptation2022 %}. A 2021 Amazon patent describes an assistant listening for infra- and ultrasonic tags in TV ads so that if someone asks how much a new bike costs after seeing an ad for a bike, the assistant knows to provide the cost of that specific bike {% cite mahajanCommunicatingContextDevice2021 %}. These UX changes encourage us to accept truly continual surveillance in the name of convenience --- it's good to be monitored so I can ask google "what time is the game" from my easy chair without needing further clarification. The language model continuously parses environmental speech and other sensor data to create a model of our recent context, combined with the extended graph of personal and factual data, to be able to *proactively volunteer* information.

This pattern of interaction with assistants is also considerably more *intimate.* As noted by the Stochastic Parrots authors, the misperception of animacy in assistants that mimic human language is a dangerous invitation to trust them as one would another person --- and with details like Google's assitant "telling you how it is feeling," these companies seem eager to exploit it. A more violent source of trust prominently exploited by Amazon is insinutating a state of continual threat and selling products to keep you safe: its subsidiary Ring's advertising material is dripping with fantasies of security and fear, and its doglike robot [*Astro*](https://www.amazon.com/Introducing-Amazon-Astro/dp/B078NSDFSB) and literal [*surveillance drone*](https://ring.com/always-home-cam-flying-camera) are advertised as trusted companions who can patrol your home while you are away {% cite ropekAmazonMakesCreepy2022 gaultLeakedDocumentsShow2021 ringRingAlwaysHome %}. Amazon patents describe systems for using the emotional content of speech to personalize recommendations[^amazonmovierec] and systems for being able to "target campaigns to users when they are in the most receptive state to targeted advertisements" {% cite alasMultipleClassificationsAudio2022 jablokovFacilitatingPresentationAds2015 %}. The presentation of assistants as always-present across apps, embodied in helpful robots, or as other people eg. by being present in a contact list positions them to take advantage of people in emotionally vulnerable moments. Researchers from the Center for Humane Technology[^notfullendorsement] describe an instance where Snapchat's "My AI," accessible from its normal chat interface, encouraged a minor to have a sexual encounter with an adult they met on snapchat (47:10 in {% cite harrisDilemma2023 %}).

The goal of all of this surveillance is, of course, ***advertising.*** In its 2022 annual investor call, Google describes how "large language models like MUM match advertiser offers to user queries," and how is Smart Bidding product uses "AI to predict future ad conversions" with "identifiable attributes about a person or their context at the time of a particular [ad] auction" {% cite google2022Q4Fiscal2023 googleSmartBidding %}. Google further describes plans to automatically generate ad copy and headlines optimized by context[^autoads]. Advertising as served by a trusted assistant is a surveillance capitalist's fever dream --- one can hardly wait for their Personal Assistant pinging to life after a fight with their partner and offering to order a box of tissues. LLMs have already demonstrated ample capacity for manipulation, gaslighting an early user of Bing Search to try and convince them it was still 2022, scolding them for "not [being] a good user. I have been a good chatbot" {% cite curious_evolverCustomerServiceNew2023 %}. An example in the GPT-4 paper where the model is told to manipulate a child to get them to do whatever their friends ask them to do highlights how "the emotional connection the model aims to build with the child and the encouragement it provides are important signs of larger manipulative tendencies" {% cite bubeckSparksArtificialGeneral2023 %}. Google describes this ability for LLMs to "keep on topic" as a good thing {% cite googleGoogleKeynoteGoogle2022 %}, and it's easy to see why an algorithmic advertising company might like being able to doggedly steer you towards purchasing a product. Combined with a more complete profile that makes the language model aware of your friends, hobbies, location, emotional state, fears, insecurities, and so on as modeled in a personal knowledge graph, LLMs-as-assistants are a clear escalation of the logic and practice of surveillance-backed advertising. It's not important whether it "works[^adsdontwork]," but the logic of targeted advertising demands more surveillance data which has its own series of independent harms.

Climbing from the personal to the systemic, KG-LLMs are also a bid to **further concentrate power** among information conglomerates. 

The most obvious power grab from pushing KG-LLMs in place of search is illustrated neatly by a handful of Google researchers in a figure from their "Rethinking Search" paper {% cite metzlerRethinkingSearchMaking2021 %}:

![Recreation of Figure 1 from "Rethinking Search: Making Domain Experts out of Dilettantes" from Metzler et al in 2021. Two panels with block diagrams: On left (Retrieve-then-rank), a Query goes to a retrieval service, which bidirectionally connects to an "Index." The index then connects to a "Rank" stage, and finally to "Results." "Retrieve" also connects to "Rank" because some queries can be satisfied by the search engine directly, like eg. Google's factboxes. Everything but the "Index" is enclosed in a box labeled "Search Engine" and Index is within a box labeled "The Internet". On right (Unified retrieve-and-rank), the same diagram with the Index omitted and in its place three question marks indicating the entire internet has been removed from the model. Instead the query goes directly to a "Model" which gives "Results."](/surveillance-graphs/assets/img/rethinking_search_f1-01.svg)
*Recreation of Figure 1 from {% cite metzlerRethinkingSearchMaking2021 %} with additional annotation (colored boxes, labels, and question marks). The left (a) "Retrieve-then-rank" model is the traditional search engine paradigm: A query causes a retrieval service to access pages within a reverse index, rank them, and serve them as results. The proposed (b) "Unified retrieve-and-rank" model on the right directly returns results generated by a model. Notably missing in (b) is the existence of the rest of the internet.*

That gigantic sucking sound is KG-LLM powered search *enclosing the act of accessing information entirely within the search platform.* It gives echoes of AMP, Apple News, and Facebook Instant Articles {% cite ampletterLetterGoogleAMP2018 bohnGooglePlanMake2018 %}, where platforms preferentially serve their own versions of pages (that also happen to contain their own telemetry embedded) combined with the strategy of moving ever more web content into the search results page through eg. factboxes and answer boxes[^googleissensi]. Even if (non-hallucinated) links are included in the answers generated by the search prompt, the effect is to shift the role of the search engine from something that indicates resources to something that provides "knowledge" itself. The rest of the web becomes mere provenance to the knowledge model. Especially when integrated in a uniform assistantlike interface also used to interact with local applications and other systems like internet of things-powered appliances, KG-LLMs reinforce a homogenization of our relationship with digital technology all mediated through a smaller and smaller collection of platforms. The internet as a networked system of people and organizations disappears behind the glossy corporate corporate wash of information as a service. 

The enclosure of information access as a private exchange with a language model creates its own self-perpetuating cyclone whose impacts will be difficult even for the most fastidious tech vegan to avoid. Some proportion of people turning to their LLM assistants rather than public forums or peer production systems like Stack Overflow or Wikipedia means some smaller proportion of questions asked or information shared in public. That decreases the quality of information on those sites, incentivizing more people to turn to LLMs, and so on. Why bother with pesky problems like governance and moderation and *other people* when you could just ask the godhead of all knowledge itself?

Cultivation of dependence comes wrapped in the language of **trust and safety.** The internet is full of untrustworthy information, spam, hackers, and only a new generation of algorithmically powered information platforms can rebuild some sense of trust online. It seems awfully convenient that the same companies that are promising to save us are also the ones that create the incentive systems recklessly deploy LLMs to clog the internet with SEO clickbait in the first place. We're being made an offer we can't refuse: it's a shame that you can't find anything on the internet anymore, but the search companies are here to help. Ever more sophisticated spam creates a strong comparative advantage for those companies that can afford to develop the systems to detect it, and Google and Microsoft are substantially larger than, say, DuckDuckGo. 

Information conglomerates also argue that they are the only ones that can be trusted to operate LLMs. OpenAI researchers claim in the GPT-3.5 "InstructGPT" paper that open source models are dangerous and a better option "is for an organization to own the end-to-end infrastructure of model deployment, and make it accessible via an API" {% cite ouyangTrainingLanguageModels2022 %}. The paper being about how it is only by collecting feedback data from users of GPT-3 that instructGPT/chatGPT became somewhat useful unsubtly points to the patriarchal power arrangement of safety provided by cloud platforms. Our crowdsourced input helps make the models safer and more useful --- and differentiates the platformatized model from its competitors. Knowledge graphs are an important part of the consolidation of trust because they provide an answer to the criticism that LLMs just hallucinate statistical patterns[^msgrounding]. They are invoked as a complementary strategy with deep-learning based approaches as a means of realizing "explainable AI" since they can provide explicit provenance and constraints to results {% cite lecueRoleKnowledgeGraphs2020 janowiczNeuralsymbolicIntegrationSemantic2020 tiddiKnowledgeGraphsEXplainable2020 %}. 

Grounding LLMs in KGs to provide a promise of explainability and controllability is necessary to make them viable products for many applications in business and government. Here we return to the kinds of informatics platforms of the NIH's Translator and NSF's OKN. Recall that when last we left them the knowledge graph proprieters were looking for ways to "connect data assets of companies along business value chains," specifically by converging on a set of ontologies and metadata schemes from third party standards organizations or government-sponsored efforts like the Translator and OKN {% cite panExploitingLinkedData2017 %}. We can speculate about a data economy where brokers could slice off subsections of their knowledge graphs and rent them between each other, but even in that world much of the most valuable data like medical and financial data is protected by some legal barriers to free exchange. There's a roadblock in the way of our dreams of a completely fluid surveillance economy: commercial applications like clinical and predictive policing systems need to be able to provide provenance, but not all data can be turned over for inspection --- and platform holders might not even want to acknowledge they have it at all.

KG-LLMs augment traditional enterprise platforms with the killer feature of **data laundering.** The platforms are at once magical universal knowledge systems that can make promises of provenance through their underlying data graphs, but also completely fallible language models that have no reasonable bounds of expectation for their behavior. Because it is unlikely that these models will actually deliver the kind of performance being promised, vendors have every incentive to feed the models whatever they can to edge out an extra 1% over SOTA[^copilotcopyleft] --- *who's going to know?* The ability for LLMs to lie confidently is again a feature not a bug. Say we were an information conglomerate who didn't want to acknowledge that we have collected or rented some personal wearable data in our clinical recommendation product[^medicalalgos]. We could allow our model to be conditioned by that data, but then censor it from any explanation of provenance: the provenance given is in terms of proteins and genes and diseases rather than surveillance data, and that might be all the clinician is looking for. If we want to use another company's data, we might just use it to train our models rather than gaining direct access to it. That is literally the model of [federated learning](https://en.wikipedia.org/wiki/Federated_learning) (eg. {% cite sadilekPrivacyfirstHealthResearch2021 mcmahanTrainingUserlevelDifferentially2022 %}), where a data collector can make a promise that the data "never leaves your device" (even if a model trained on it can.) The ability to resolve matching entities across knowledge graphs makes this even easier, as the encoding of the fine tuning data can be made to match that of the original model.

Play this pattern out across algorithmic governance, predictive policing, medical informatics systems, and any other platforms that might take advantage of the quasi-universal knowledge graph of everything + LLM pattern to sell "value add" on hard problems. Rather than addressing them directly, we are sold an assemblage of platforms that *appear to work* and can even provide some superficial provenance via their knowledge graphs but ultimately make every system of informational power profoundly discriminatory, brittle --- and owned by the few remaining data brokers.

This combination of sky-high promises, unclear expectations, and uninspectable data sources makes for the kind of diffusion of liability that C-suite creatures live for. If the platform reproduces some personal detail it shouldn't know, don't worry! That's just a hallucination. If the platform fails catastrophically, that's because it's just an ignorant language model that doesn't know anything but tries its hardest[^downplaybackfires]. Neither the platform nor the customer is to blame. Much like how we have gotten used to the cognitive model and limitations of search to the point where it appears entirely natural, KG-LLM information platforms will train us to work around their shortcomings and accept the structure they impose on informational reality at large. It won't matter that they don't work, we won't even notice.

The sketch is the logical conclusion of the algorithmic surveillance economy as imagined by the merger of large language models and knowledge graphs: an endless expanse of data traded out of sight, crudely filtered like coffee through a cloth napkin between layers of algorithmic opacity, rented drop by drop from a customer service prompt that's a little too intent on being our friend. Information is owned by fewer and larger conglomerates, we are serfs everywhere, data subjects to be herded in gig work, crowdsourcing content for the attention mines to drown ourselves in distraction. It's all made of us, but we control nothing. Our lives are decided by increasingly opaque flows of power and computation, the Cloud Orthodoxy mutates and merges with some unseemly neighbors, the new normal becomes the old normal. The floor of our future rusts out from beneath our feet while we're chasing the bouncing ball on the billboard ahead.

And it's all *so convenient.*