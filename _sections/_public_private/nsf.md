### NSF: Open Knowledge Network

While the NIH builds a set of universal knowledge graphs for biomedical information, the NSF is building them for everything else. Its Open Knowledge Network (OKN) project intends to "provide an essential public-data infrastructure for enabling an AI-driven future." {% cite baruOpenKnowledgeNetwork2022 %} Compared to the Translator, the OKN pulls punches for neither its utopian promises nor obvious risks. Some sections of its [roadmap](https://web.archive.org/web/20221028095757/https://nsf-gov-resources.nsf.gov/2022-09/OKN%20Roadmap%20-%20Report_v03.pdf) are written in the breathless tenor of Big Data solutionism, claiming that "harnessing the vast amounts of data generated in every sphere of life and transforming them into useful, actionable information and knowledge is crucial to the efficient functioning of a modern society" {% cite baruOpenKnowledgeNetwork2022 %}. Without mincing words, the OKN intends to make a Universal Knowledge Graph of Everything. The recipe is familiar: a) make authoritative schemas for everything, b) link them all together, c) ingest data from as many sources as possible at whatever quality available, d) integrate private with public data e) put it all in the cloud! (p. 18-19 "Creating an OKN" {% cite bigdatainteragencyworkinggroupOpenKnowledgeNetwork2018 %}). 

The project was initially proposed in 2017, went through two [cohorts](https://beta.nsf.gov/funding/initiatives/convergence-accelerator/portfolio) of projects within the [NSF Convergence Accelerator](https://beta.nsf.gov/funding/initiatives/convergence-accelerator/portfolio) in 2019 and 2020[^NSFconvergence], and [invited a broader submission](https://www.nsf.gov/pubs/2022/nsf22017/nsf22017.jsp) of proposals in November 2021 {% cite nationalsciencefoundationNSF22017Dear2021 %}. The roadmap comes at the end of a series of workshops in 2022 intended to scope and outline the OKN so there is still very little public evidence of its progress to evaluate[^spoke], but along with the Translator, what is available tells the story of an emerging consensus for public data infrastructures.

Its domain is much broader than the Translator, and is unmistakably bound up in both the United States Federal Government's military and political interests in Artificial Intelligence[^ainationalsecurity]  {% cite nationalsecuritycommissiononartificialintelligenceFinalReport2021 %} and the information economy's interests in making a universal space where all information can be bought and sold with minimal friction[^bigdataworkinggroup] {% cite bigdatainteragencyworkinggroupOpenKnowledgeNetwork2018 %}. Where the Translator has the near-inevitable risk of being captured by information conglomerates, through the euphemism of "public private partnership" the OKN makes clear it intends capture by for-profit entities as part of its design: for example, the team behind the SPOKE biomedical knowledge network immediately spun off a for-profit startup to [sell the graph as a cloud service](https://www.matebioservices.com/spoke-cloud) {% cite matebioservicesinc.SPOKECloud2021 %}, abandoning further UX development of its [publicly accessible demo](https://spoke.rbvi.ucsf.edu/).

They OKN describes its work along "vertical" and "horizontal" dimensions, where "vertical" applications refer to specific uses or domains like energy or health data, and "horizontal" themes like technologies and governance are shared across all domains. The collection of "vertical" topics identified in the 2022 roadmap hint at the effectively unbounded scope of the OKN: accelerated capitalism via supply chain logistics, more tightly integrated weapons development, a handful of climate change projects, an omniscient financial system, and so on. Each imagines the primary problem in a given domain not as structural exploitation or injustice, but a lack of data[^systemsengineers].

The "vertical" topical working groups in the 2022 roadmap centered on an algorithmic justice system are particularly illustrative: An **Integrated Justice Platform** group describes the need for greater surveillance across every contact people have with the US Justice System in a wish list of data sources that should be integrated - arrest and booking, jail, trial, prosecution, and the rest. A **Decarceration** group[^boozallen] describes extending that surveillance through to the rest of incarcerated people's lives after they are released - rehab, parole, foster care, shelters, public services, etc. A **Homelessness** group intends to track unhoused people in order to match them to available resources. A **Decision Support for Government**[^smartcity] group describes bundling up these and other data sources into platforms for making "data driven decisions" on topics including crime and policing.

On their own, each of these groups describes noble goals: decreasing bias in the justice system, providing resources to formerly incarcerated or unhoused people, making government decisions more efficient. Taken together, however, the projects describe a panoptical surveillance system that wouldn't even need to be reconfigured to be used for algorithmically-enhanced oppression. I doubt any of the researchers in these groups intend for their work to be used for state violence, but *Palantir doesn't care what academics intended their tools to be used for[^palantirexternaldata].* 

The motivations behind integrating government data sources and automating public benefit delivery cannot overcome the context of systemic oppression they are embedded within. Group H, the "Homelessness OKN" group, takes particular effort[^sprint] to focus on the needs of the unhoused and address the potential risks of "track[ing] homelessness in real time, [and] identify[ing] available homelessness programs and services," but misses the already-real harms of similar prior efforts. Virginia Eubanks describes how Los Angeles County's Coordinated Entry System --- a program very much like that described by group H, intended to match unhoused people with housing supply by integrating previously siloed data systems --- operates as a sophisticated mechanism of control and punishment: 

> For Gary Boatwright and tens of thousands of others who have not been matched with any services, coordinated entry seems to collect increasingly sensitive, intrusive data to track their movements and behavior, but doesn't offer anything in return. [...] Moreover, the pattern of increased data collection, sharing, and surveillance reinforces the criminalization of the unhoused, if only because **so many of the basic conditions of being homeless are also officially crimes.** [...] The tickets turn into warrants, and then law enforcement has further reason to search the databases to find “fugitives.” Thus, **data collection, storage, and sharing in homeless service programs are often starting points in a process that criminalizes the poor.** [...]
> 
> Further integrating programs aimed at providing economic security and those focused on crime control threatens to turn routine survival strategies of those living in extreme poverty into crimes. **The constant data collection from a vast array of high-tech tools wielded by homeless services, business improvement districts, and law enforcement create what Skid Row residents perceive as a net of constraint that influences their every decision.** Daily, they feel encouraged to self-deport or self-imprison. Those living outdoors in encampments feel pressured to constantly be on the move. Those housed in SROs or permanent supportive housing feel equally intense pressure to stay inside and out of the public eye. [...] **Coordinated entry is not just a system for managing information or matching demand to supply. It is a surveillance system for sorting and criminalizing the poor.** {% cite eubanksAutomatingInequalityHow2019 %}

It is impossible to consider integrated data in government without confronting the reality of algorithmic policing. Under its Strategic Plan goal of "Realiz[ing] Tomorrow's Government Today" Los Angeles County has already been integrating its information systems, including creating a unified system of law enforcement and other public service data "to identify super utilizers of justice and health system resources"[^cfive] {% cite chiefexecutiveofficecountyoflosangelesStrategicPlanGoal2022 farahaniLinkingPublicSafety2016 %}. Many police departments --- including the LAPD --- already have access to the kind of linked data ecosystems described by the OKN by renting them from private data brokers like Palantir {% cite braynePredictSurveilData2020 lamdanDefundPoliceDefund2020 %}. These data infrastructures facilitate the well-described feedback loop of predictive policing, where areas already subject to historical economic and racist violence are classified as "high-crime areas," more police are concentrated there, in turn causing them to measure or create more crime[^acab] {% cite braynePredictSurveilData2020 guarigliaTechnologyCanPredict2020  stoplapdspyingcoalitionRacialTerrorWhite2021 kathleenTargeted2020 stoplapdspyingcoalitionBulletHitsBody2018 stoplapdspyingcoalitionLetter28Professors2019 castelvecchiMathematiciansUrgeColleagues2020 %}. The reformist idea that more data will help us "police the police" is belied by the resolute history of more data allowing the police to innovate on information asymmetries to create new expressions of power {% cite hongPredictionExtractionDiscretion2022 stoplapdspyingcoalitionFUCKPOLICETRUST2020 %}.

The critical difference between prior infrastructures and those imagined by the OKN is that they are explicitly designed to be linked into a continuous network of data that enables the same kind of data-driven decisionmaking that drives predictive policing for *any* system. We should not be imagining the utterly mechanistic bureaucracy of *Kafka* here, but rather the deeply expressive and personal exercise of power of Terry Gilliam's *Brazil.* Widespread algorithmic governance doesn't necessarily look like a faceless bureaucracy where all decisions are made by a computer, existing algorithmic systems like predictive policing and the working conditions at Amazon warehouses retain the very human domain of *discretion* (see {% cite hongPredictionExtractionDiscretion2022 %}). The algorithms and seemingly open infrastructures of these two projects purport themselves as objective and egalitarian, but who they are built for, who gets to provides the inputs, and who decides which outputs matter make their reality very different. 

A report from Wired and Lighthouse Reports that gained unprecedented access to an algorithmic social service system created by Accenture for the city of Rotterdam shows how the discretion of caseworkers and a purportedly "objective" algorithm together create a profoundly discriminatory system {% cite constantarasSuspicionMachine2023 braunSuspicionMachinesMethodology2023 %}. Caseworkers make subjective determinations like an applicant showing signs of low self-esteem or whether they can "deal with pressure" and feed them along with characteristics like age and gender into an opaque set of decision trees to determine whether they should be investigated for benefits fraud. The opacity of the system makes it rich with opportunities for discretionary bias that, again, can be both intentional and unintentional. For example, the mere *presence* of a comment on motivation or attitude increases ones likelihood of being flagged for investigation, even if that comment is positive. Intentional and unintentional welfare fraud are undifferentiated in the training data, making language barriers --- a source of accidental fraud from not understanding the system --- a primary determinant of investigation. In the case of the OKN, merging data from many governmental systems under the aegis of algorithmic fairness could do precisely the opposite: expanding the points of discretionary control where opaque decisions in input data or application of an algorithm can have long range impacts on governmental outcomes. 

