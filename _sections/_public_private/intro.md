### Unqualified Openness Considered Harmful

If the problem is information conglomerates stockpiling a massive quantity of proprietary data and renting use of it, isn't "open data" the answer? "Openness," including open source, open standards, and open data, is a subtle tool that can be used both to dissolve and reinforce economic and political power and is particularly ill-suited as a counter-strategy for corporate knowledge graphs .

Free and open source software, with its noble (and decidedly non-monolithic {% cite liuFreedomIsnFree2018 %}) goal of creating an ecosystem of free[^freesoftware] software, is a means by which large information companies can harvest the commons and outsource labor costs {% cite warkHackerManifesto2004 goldsmithOriginalSinFree2019 hallidayOpenSourceNot2018 hunterReclaimingComputingCommons2016 hornPostOpenSource2020 %}. There are countless examples of FOSS developers maintaing software widely used by companies making billions of dollars for little or no compensation - eg. [core-js](https://github.com/zloirock/core-js/blob/master/docs/2023-02-14-so-whats-next.md) {% cite pushkarevWhatNext2023 %}, [OpenSSL](https://veridicalsystems.com/blog/of-money-responsibility-and-pride/index.html) {% cite marquessSpeedsFeedsMoney2014 %}, leftpad {% cite gallagherRagequitCoderUnpublished2016 %}, [PLC4X](https://github.com/chrisdutz/blog/blob/main/plc4x/free-trial-expired.adoc) {% cite dutzYourFreeTrial2022 %} and so on. When an information company releases or supports an open source project it is rarely an act of altruism. The effect is to prevent another company from profiting from a proprietary version of that technology, signal virtue, drive recruitment, and create a centralized point to concentrate donated labor. Microsoft, a famously [good actor](https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish) in software, took this several steps further with GitHub, VSCode, and later Copilot, capturing a large chunk of the software development *process* in order to trick programmers to be the "[humans in the loop](https://twitter.com/json_dirs/status/1410897161277956097)" refining the neural network to write code and dilute their labor power {% cite butterickGitHubCopilotInvestigation2022 butterickGitHubCopilotLitigation2022 olearyVSCodeWhat2022 VSCodiumOpenSource %}.

"[Peer production](https://en.wikipedia.org/wiki/Peer_production)" models, a more generic term for public collaboration that includes FOSS, has similar discontents. The related term "crowdsource[^crowdsource]" quite literally describes a patronizing means of harvesting free labor via some typically gamified platform. Wikipedia is perhaps the most well-known example of peer production[^wikipedia], and it too struggles with its position as a resource to be harvested by information conglomerates. In 2015, the increasing prevalence of Google's information boxes caused a substantial decline in wikipedia pageviews {% cite UserTalkJimbo2015 hinkisGoogleSteals5502015  %} as its information was harvested into Google's knowledge graph, and a "will she, won't she" search engine arguably intended to avoid dependence on Google was at the heart of its 2014-2016 leadership crisis {% cite whiteWikimediaTimelineEvents2016 buetlerSearchDestroyKnowledge2016 %}. While shuttering Freebase, Google donated a substantial amount of money to kickstart its successor {% cite pellissiertanonFreebaseWikidataGreat2016 %} Wikidata, presumably as a means of crowdsourcing the curation of its knowledge graph {% cite wikimediameta-wikiGoogleMeta GoogleStakeWikidata2019 vrandecicWikidataFreeCollaborative2014 %}.

"Open" standards are yet another fraught domain of openness. For an example within academia, the seemingly-open Digital Object Identifier (DOI) system was concocted as a means for [publishers to retain control of indexing research](https://jon-e.net/infrastructure/#seemingly-prosocial-protocols-can-be-used-by-industries-to-preem), avoiding the impact of the proposed free repository PubMedCentral and the high overhead of linking documents between publishers[^linkingagreements] (see sec. 3.1.1 in {% cite saundersDecentralizedInfrastructureNeuro2022 %}). The nonprofit standards body [NISO](https://www.niso.org)'s standards for indicating journal article versions {% cite nisoRP82008JournalArticle2008 %} and licensing {% cite nisoRP222021AccessLicense2021 %} are used by publishers to enforce their intellectual property monopolies and programmatically scour the web to prevent free access to publicly funded information {% cite carpenterNewArticleSharing2021 %}.

Schema.org, a standard intended to be the generic interchange ontology of the web, is another emblem of enclosure of the semantic web. Its introduction at the SemTech 2011 conference was cause for a rare point of agreement[^rdfavmicroformats] between the then-warring maintainers of RDFa and Microformats: "folks, it's wrong for Google to dictate vocabularies, let's not lose sight of that" {% cite SemTech2011BOF2011 %}. Though ostensibly open, its structure and emphases have been roundly criticized, eg. having a eurocentric bias towards commercially valuable information {% cite iliadisOneSchemaRule2023 %}. It encourages website maintainers to embed Schema.org annotations in their pages in exchange for a boost in search rankings --- which Google then embeds in its infoboxes, driving down page views. More fundamentally it cements the notion that Linked Data is something that we are only intended to use to make our information more available to some search engine crawler rather than make use of for ourselves: "In general, the design decisions place more of the burden on consumers of the markup" {% cite guhaSchemaOrgEvolution2015 %}. It encodes the notion that there should be one "neutral" means of representing information for one (or a few) global search engines to understand, rather than for local negotiation over meaning. According to the transcribed Q&A after its 2011 announcement, the Google representatives characterized the creation of authoring tools like those created to make creative use of HTML more accessible as a potential "alternative path," but then dismissed the notion of improved tooling as "impossible" {% cite hawkeNotesSessionSemTech2011 %}. 

Clearly, on its own, mere "openness" is no guarantee of virtue, and socio-technological systems must always be evaluated in their broader context: *what is open? why? who benefits?* Open source, open standards, and peer production models do not inherently challenge the rent-seeking behavior of information conglomerates, but can instead facilitate it. 

In particular, the maintainers of corporate knowledge graphs want to reduce labor duplication by making use of some public knowledge graph that they can then "add value" to with shades of proprietary and personal data (emphasis mine):

> In a case like IBM clients, who build their own custom knowledge graphs, **the clients are not expected to tell the graph about basic knowledge.** For example, a cancer researcher is not going to teach the knowledge graph that skin is a form of tissue, or that St. Jude is a hospital in Memphis, Tennessee. This is known as **“general knowledge,”** captured in a general knowledge graph. **The next level of information is knowledge that is well known to anybody in the domain**—for example, carcinoma is a form of cancer or NHL more often stands for non-Hodgkin lymphoma than National Hockey League in some contexts it may still mean that—say, in the patient record of an NHL player). **The client should need to input only the private and confidential knowledge** or any knowledge that the system does not yet know. {% cite noyIndustryscaleKnowledgeGraphs2019 %}

The creation of a collection of more domain-specific ontologies and tooling for ingesting previously unstructured data would allow for a new kind of globally linked knowledge graph ecosystem --- making use of a broader range of publicly-available data, as well as facilitating new markets for renting access to interoperable data. Five information conglomerates conclude their joint paper on knowledge graphs accordingly:

> The natural question from our discussion in this article is whether different knowledge graphs can someday share certain core elements, such as descriptions of people, places, and similar entities. {% cite noyIndustryscaleKnowledgeGraphs2019 %}

Having such standards be under the stewardship of ostensibly neutral and open third-parties provides cover for powerful actors exerting their influence and helps overcome the initial energy barrier to realizing network effects from their broad use {% cite wiegmannMultiModeStandardisationCritical2017 heiresInternationalOrganizationStandardization2008 %}. Peter Mika, the director of Semantic Search at Yahoo Labs, describes this need for third-party intervention in domain-specific standards:

> A natural next step for Knowledge Graphs is to **extend beyond the boundaries of organisations,** connecting data assets of companies along business value chains. This process is still at an early stage, and **there is a need for trade associations or industry-specific standards organisations to step in,** especially when it comes to developing shared entity identifier schemes. {% cite panExploitingLinkedData2017 %}

As with search, we should be particularly wary of information infrastructures that are *technically* open[^diygoogle] but embed design logics that preserve the hegemony of the organizations that have the resources to make use of them. The existing organization of industrial knowledge graphs as chimeric "data + compute" models give a hint at what we might look for in public knowledge graphs: the data is open, but to make use of it we have to rely on some proprietary algorithm or cloud infrastructure.

Unfortunately, that is exactly what at least two US Federal agencies have in mind: the NIH and NSF are both in the thick of engineering cloud-based knowledge graph infrastructures and domain-specific ontologies with all the trappings of technology that fills the stated needs of information conglomerates at the expense of the people it is outwardly intended to serve. I assume that the researchers and engineers working on these projects are doing so with the best of intentions. The object of criticism is not the individuals within these projects, but the ideologies and systems they are embedded within. I will describe those efforts and their already apparent harms as a way of understanding how these technologies illustrate and reinforce the dominance of the existing corporate informational ecosystem --- and to articulate an alternative.
