General principle of platformatization doing odd things with proprietariness - eg. the DOI was to facilitate extraction, not alleviate it. Information conglomerates exploit and EEE open source all the time. There is a basic dichotomy between "the code" or "the data" and the means by which it is possible to make use of it. In KGs specifically there is a certain amount of computation that is necessary within the networks, so even if the data is all public, the system can effectively be proprietary if it is impossible to use or one needs really sophisticated proprietary algorithms to make sense of it


Broad principles
- Strategic use of "openness" when it facilitates greater market control and to prevent someone else from capturing a particular element of the technology.	
	- Same thing as with the DOI: the linking agreements were killing us! Get some barrier to commerce out of the way (ontology discontinuity) so that the commerce can intensify, not so that it can abate
	- re: crowdsourcing semantics: {% cite allhutterWorkingOntologistsHighQuality2019 %}
- Need to create a larger space of shared 'public data' that the companies can then add their 'private data' to: . 
- include wikipedia example, how they hated when google just was siphoning off traffic from them so they do it in a more different way.
- Yes, wikidata exists, but it's extremely carefully managed with the intention of creating "facts" that don't embarass google {% cite chahOKGoogleWhat2018 %}

> In a case like IBM clients, who build their own custom knowledge graphs, the clients are not expected to tell the graph about basic knowledge. For example, a cancer researcher is not going to teach the knowledge graph that skin is a form of tissue, or that St. Jude is a hospital in Memphis, Tennessee. This is known as “general knowledge,” captured in a general knowledge graph. The next level of information is knowledge that is well known to anybody in the domain—for example, carcinoma is a form of cancer or NHL more often stands for nonHodgkin lymphoma than National Hockey League in some contexts it may still mean that—say, in the patient record of an NHL player). The client should need to input only the private and confidential knowledge or any knowledge that the system does not yet know. Isolation, federation, and online updates of the base and domain layers are some of the major issues that surface because of this requirement. 
>
> [...]
> 
> **The natural question from our discussion in this article is whether different knowledge graphs can someday share certain core elements, such as descriptions of people, places, and similar entities.** {% cite noyIndustryscaleKnowledgeGraphs2019 %}

> Peter Mika: A natural next step for Knowledge Graphs is to extend beyond the boundaries of organisations, connecting data assets of companies along business value chains. This process is still at an early stage, and there is a need for trade associations or industry-specific standards organisations to step in, especially when it comes to developing shared entity identifier schemes. {% cite panExploitingLinkedData2017 %}

"Director of Semantic Search, Yahoo Lab"

> With a large-scale knowledge graph, developers can build high-dimensional representations of entities and relations. The resulting embeddings will greatly benefit many machine-learning, NLP, and AI tasks as sources of features and constraints, and can form the basis for more sophisticated inferences and ways to curate training data. {% cite noyIndustryscaleKnowledgeGraphs2019 %}

When Freebase was closed, its graph was (partially) integrated into Wikidata {% cite pellissiertanonFreebaseWikidataGreat2016 WikidataWikiProjectFreebase %}, which is the dominant collaborative public knowledge graph today. 

---

Linked Open Data Vs. Knowledge Graphs - KGs are products! LOD is a messy and wily ball of data!

The pattern of capturing public infrastructure 

Dividing publicly curated knowledge like facutla information that anyone can know along with privately available knowledge eg. that that is available to anyone vs. private surveillance data - it's good to have the public information be complete, you a) own the compute to make use of it and b) can merge it with your private surveillance information in order to ~ unlock new products ~ . 

Also you can't ignore the sheer dominance of literally owning the hardware that it all runs on... which also structures the nature of these projects.

All the public stuff is great, but they don't have a good mechanism for how to collaborate on private contents of graphs, so that's why the government graphs are so worrying in particular

- Open source data, proprietary compute.
	- eg. wikidata, github copilot, pattern of corporations exploiting open source 
	- The transition of freebase as an autonomous knowledge graph to one owned by google, shut down, and then a lot of that labor transitioned to wikidata: {% cite pellissiertanonFreebaseWikidataGreat2016 chahOKGoogleWhat2018 %}