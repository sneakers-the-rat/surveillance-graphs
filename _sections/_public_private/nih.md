### NIH: The Biomedical Translator

<div class="note">
**Note:**

This section is reproduced from, focuses, and expands on "[Linked Data or Surveillance Capitalism?](https://jon-e.net/infrastructure/#linked-data-or-surveillance-capitalism)" from {% cite saundersDecentralizedInfrastructureNeuro2022 %}.
</div>

The NIH's Biomedical Data Translator[^translator] project was initially described in its 2016 Strategic Plan for Data Science as a means of translating between biomedical data formats:

> Through its Biomedical Data Translator program, the National Center for Advancing Translational Sciences (NCATS) is supporting research to develop ways to connect conventionally separated data types to one another to make them more useful for researchers and the public. {% cite NIHStrategicPlan2018 %}

The original [funding statement from 2016](https://web.archive.org/web/20210709100523/https://ncats.nih.gov/news/releases/2016/feasibility-assessment-translator) is similarly humble, and press releases [through 2017](https://web.archive.org/web/20210709171335/https://ncats.nih.gov/pubs/features/translator) also speak mostly in terms of querying the data -- though some ambition begins to creep in. By 2019, the vision for the project had shifted from *translating* between data types into the realm of heterogeneous linkages in some meta-level system for linking and *reasoning* over them. 

In their piece "Toward a Universal Biomedical Translator," then in a feasibility assessment phase, the members of the Translator Consortium assert that universal translation between biomedical data is impossible[^impossibledata]{% cite consortiumUniversalBiomedicalData2019 %}. The impossibility they saw was not that of conflicting political demands on the structure of organization (as per {% cite bowkerSortingThingsOut1999 %}), but of the sheer numeracy of the data and vocabularies needed to describe them. The risk posed by a lack of a universal "language" was not being able to index all possible data, rather than inaccuracy or inequity[^babel].

Undaunted by their stated belief in the impossibility of a universalizing ontology, the Consortium created one in their [biolink](https://biolink.github.io/biolink-model/docs/) model[^biolinkpaper] {% cite bruskiewichBiolinkBiolinkmodel2021 unniBiolinkModelUniversal2022 %}. Biolink consists of a hierarchy of general[^generality] classes: eg. a [BiologicalEntity](https://biolink.github.io/biolink-model/docs/BiologicalEntity.html) like a [Gene](https://biolink.github.io/biolink-model/docs/Gene.html), or a [ChemicalEntity](https://biolink.github.io/biolink-model/docs/ChemicalEntity.html) like a [Drug](https://biolink.github.io/biolink-model/docs/Drug.html). Classes can then linked by any number of properties, or "Slots[^slots]," like a therapeutic procedure that [treats](https://biolink.github.io/biolink-model/docs/treats.html) a disease. 

Biolink was designed to be a sort of "meta ontology," or a means of mapping different domain-specific biomedical ontologies onto a common vocabulary[^tooling]. This design reflects the structure of the rest of the Translator ecosystem: the interaction with domain-specific ontologies, the kinds of data sources it uses, and the way that end users are expected to interact with the Translator.

As a meta-ontology, biolink is targeted towards "meta data." Rather than accomodating "raw data[^norawdata]," biolink is expected to operate at the level of "knowledge," or "generally accepted, universal assertions derived from the accumulation of information" {% cite fechoProgressUniversalBiomedical2022 %}: this procedure treats that disease, this chemical interacts with that one, etc. 

The primary way Biolink is used within the Translator is to structure a [registry of database APIs](http://www.smart-api.info/registry), each called a "Knowledge Source." Knowledge Sources use biolink to declare that they are able to provide assertions about a particular set of classes or slots, like [drugs that affect genetic expression](http://www.smart-api.info/ui/adf20dd6ff23dfe18e8e012bde686e31), which makes them part of the Translator's distributed [Knowledge Graph](http://www.smart-api.info/portal/translator/metakg). The Translator project, in this universalizing impulse, recapitulates some of the early beliefs of the Semantic Web updated with some of the techniques of Linked Data. Since acquiring Knowledge is just a matter of creating the right tools rather than a social process, [NIH RePORTER](https://reporter.nih.gov/search/DShVUhB_ZUq0X5UWFjy5WQ/projects?shared=true) shows a series of grants for small councils of experts to create domain-specific ontologies and Knowledge Sources.

This structure strongly constrains who is intended to be able to contribute to the Translator: highly curated biomedical informatics platforms, rather than basic researchers. This, in turn, reflects deeper beliefs about the nature of information within the Translator ecosystem: "knowledge" is not a social, contextual, or dialogical phenomenon, but a "natural resource" that can be [mined](https://reporter.nih.gov/project-details/10548337) from information that is "out there." A scientific paper is a neutral carrier of a factual link between entities. The meaning of "translation," in some uses, has shifted from translating *between data formats*, to *"translating information into knowledge"* {% cite consortiumUniversalBiomedicalData2019 %}. This is, of course, the ideology of Big Data: "when heterogeneous networks are connected at a massive scale, new knowledge can be extracted as an emergent property of the network" {% cite morrisScalablePrecisionMedicine2023 %}.  The Translator imagines itself as a refinery, converting crude data into knowledge that can fuel platforms.

The platforms that the translator imagines are means by which plain language queries can be translated into graph queries and have answers returned by some algorithmic "reasoning agent" that queries the Knowledge Providers and synthesizes a response {% cite renaissancecomputinginstituterenciBiomedicalDataTranslator2022 renaissancecomputinginstituterenciUseCasesShow2022 goelExplanationContainerCaseBased2021 unniBiolinkModelUniversal2022 hailuNIHfundedProjectAims2019 %}. We are not intended to use the data from Knowledge Providers directly, as it is likely to be incomplete or conflicting. Instead, the imagined use is as a recommendation system for researchers to target their research or for doctors to render care.

Several pilot experiments have demonstrated combining some aggregated patient records with the broader knowledge graph in order to, eg. identify new risk markers for disease {% cite morrisScalablePrecisionMedicine2023 nelsonEmbeddingElectronicHealth2021 translatorconsortiumClinicalDataServices2020 nelsonIntegratingBiomedicalResearch2019 %}. These systems layer personal records underneath "general" biomedical information like drug interactions and biological processes and use the extended information from the graph to infer information both about the nature of the disease and the patient. It is only with the inclusion of patient records into the knowledge graph that it becomes possible to use in a clinical setting: for even basic queries like "which drugs treat this disease" one has to be aware of patient qualities like allergies and comorbid conditions. To know how to treat the generic diagnosis of "gender dysphoria," one needs to know which gender the patient is experiencing dysphoria about. The logic of knowledge graph isn't just hungry for *some* personal medical data, the promise of the knowledge graph is that more data **always** improves the computations performed on it[^moredata]. 

Why might we be critical about the NIH funding a series of projects to unify biomedical and personal health data in some universalized, platformatized knowledge graph? In short: because it won't work and will inevitably be captured by the surveillance industry.

First, as with any machine-learning based system, the algorithm can only reflect the implicit structure of its creation, including the beliefs and values of its architects {% cite birhaneValuesEncodedMachine2022 birhaneAlgorithmicInjusticeRelational2021 %}, its training data and accompanying bias {% cite birhaneMultimodalDatasetsMisogyny2021 %}, and so on. The "mass of data" approach ML tools lend themselves to, in this case, querying hundreds of independently operated databases, makes dissecting the provenance of every entry from every data provider effectively impossible. For example, one of the providers, [mydisease.info](https://mydisease.info) was more than happy to respond to a query for the outmoded definition of "transsexualism" as a disease {% cite ramTransphobiaEncodedExamination2021 %} along with a list of genes and variants that supposedly "cause" it - [see for yourself](http://mydisease.info/v1/query?q=%22DOID%3A10919%22). At the time of the search, tracing the source of that entry first led to the disease ontology [DOID:1234](https://web.archive.org/web/20211007053446/https://www.ebi.ac.uk/ols/ontologies/doid/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FDOID_1234), which has an [official IRI](http://purl.obolibrary.org/obo/doid.owl), but in this case was being served by a graph aggregator [Ontobee](http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234) ([Archive Link](https://web.archive.org/web/20210923110103/http://www.ontobee.org/ontology/DOID?iri=http://purl.obolibrary.org/obo/DOID_1234)), which in turn listed this [unofficial github repository](https://github.com/jannahastings/mental-functioning-ontology) **maintained by a single person** as its source[^ipredit]. This is, presumably, the fragility and inconsistency in input data that the machine learning layer is intended to putty over.

If the graph encodes being transgender as a disease, it is not farfetched to imagine the ranking system attempting to "cure" it --- the entire appeal of the graph model for precision medicine is making use of the extended structure in the graph. In a seemingly prerelease version of the translator's query engine, ARAX, it does just that: in [a query for entities with a `biolink:treats` link to gender dysphoria](https://web.archive.org/web/20220828011010/https://arax.rtx.ai/?r=e891e6e6-44fd-4684-9d36-f94e3e81b554)[^araxtrans], it ranks the standard therapeutics {% cite deutschOverviewFeminizingHormone2016 deutschOverviewMasculinizingHormone2016 %} Testosterone and Estradiol 6th and 10th of 11, respectively --- behind a recommendation for Lithium (4th) and Pimozide (5th) due to an automated text scrape of [two](https://pubmed.ncbi.nlm.nih.gov/2114800/) conversion therapy [papers](https://pubmed.ncbi.nlm.nih.gov/8839957/)[^dateextract]. Queries to ARAX for [treatments for gender identity disorder](https://web.archive.org/web/20220828011112/https://arax.ncats.io/?r=52703) helpfully yielded "zinc" and "water," offering a paper from the translator group that describes automated drug recommendation as the only provenance {% cite womackLeveragingDistributedBiomedical2019 %}. A query for treatments for `DOID:1233` "[transvestism](https://web.archive.org/web/20221207013845/https://arax.rtx.ai/?r=81249a42-b300-4dcf-94c9-7a9fe2f78237)" was predictably troubling. The [ROBOKOP](https://robokop.renci.org/answer) {% cite bizonROBOKOPKGKGB2019 %} query engine behaved similarly, answering [a query for genes associated with]({{ "/data/ROBOKOP_message.json" | relative_url }}) gender dysphoria with exclusively trivial or incorrect responses[^robokopdidntwork].

Who is supposed to fix incorrect or harmful query responses? The centralized structure of the Translator's Knowledge Providers and query engines now form a small group responsible for curating the entire structure of biomedical information. The curation process could be "crowdsourced" to allow affected communities to suggest improvements, but the platformatized nature of the Translator always leaves the final say with some opaque string of platform holders. The Consortium also describes a system whereby the algorithm is continuously updated based on usage of results in research or clinical practice {% cite consortiumUniversalBiomedicalData2019 %}, which stands to magnify the problem of algorithmic bias by uncritically treating harmful treatment and research practices as training data.

These problems hint at the likely fate of the Translator project. Rather than integrating into the daily practice of researchers, the centralized process of creating Knowledge Providers can only be maintained for as long as the grant funding for the Translator project lasts. When [queried](https://arax.rtx.ai) at the time of writing, of the 25 knowledge providers that were responsive to information about "Anything that is related to the common cold," 22 were unresponsive or timed out. How the Translator works as constructed by its architects is almost irrelevant compared to the question of what happens to it *after the project ends.* Its platform structure and strong dependence on surveillance data as input make it a perfect target for the circling surveillance conglomerates.

Linking biomedical and patient data in a single platform is a natural route towards a multisided market where records management apps are sold to patients, treatment recommendation systems are sold to clinicians, research tools and advertising opportunities are sold to pharmaceutical companies, risk metrics are sold to insurance companies, and so on. The contours of this market are already clear.

As a non-exhaustive set of examples:

- I have already described **RELX**'s interest in personal biomedical data. Their 2022 Annual Report {% cite relxAnnualReport20222023 %} is the first year where they explicitly describe their entrance into the patient data market[^RELXmedicaldata]. RELX is a particularly worrying example because of their established roles among academics, medical workers, and insurance providers. 
- **Amazon** already has a broad home surveillance portfolio {% cite bridgesAmazonRingLargest2021 %}, and has been aggressively expanding into health technology {% cite AWSAnnouncesAWS2021 %} and even literally providing [health care](https://amazon.care/) {% cite fingasAmazonOfficiallyBecomes2023 lermanAmazonBuiltIts2021 %}, which could be particularly dangerous with the uploading of all scientific and medical data onto AWS with entirely unenforceable promises of data privacy through NIH's STRIDES program {% cite quinnYouCanTrust2021 %}. 
- **Google** already includes medical conditions in its surveillance-backed advertising profiles {% cite krashinskyGoogleBrokeCanada2014 bharatGeneratingUserInformation2005 %}, and is edging its way into wearable health data with eg. its acquisition of FitBit {% cite bourreauGoogleFitbitWill2020 %}. It also already has a system, Med-PALM, for biomedical question answering based on large language models {% cite piferGooglePlansBoost2023 matiasOurLatestHealth2023 singhalLargeLanguageModels2022 %}. Search is a primary entrypoint for many people searching for health information, and Google presumably would be more than happy to merge that data with a generalized biomedical knowledge graph.
- **Apple** already has a matured Health ecosystem of apps and services for both patients, clinicians, and researchers {% cite appleEmpoweringPeopleLive2022 appleHealthcare %} and has a similar exposure to relevant data and control of platforms (iOS, watchOS) to make use of it, though they have marketed themselves in the surveillance space as a defender of privacy.
- Of course **Microsoft** {% cite sinhaOverviewMicrosoftAcademic2015 %} and **IBM** {% cite chenIBMWatsonHow2016 %} are also in play.

The design of the Translator project reflects the prevailing logic of the surveillance economy as powered by knowledge graphs, and is poised to be swallowed up by it. Rather than a means for us to collectively make sense together, they have imagined a cloud-driven system where a small group of experts wave a wand of unknowable algorithms over a bulging plastic trash bag of data to pull out the Magic Knowledge Rabbit. The noble intention of making a generalized biomedical knowledge graph for the public good is unlikely to be realized, but in the process the NIH will have funded facilitating technologies and standards to allow the merger of personal medical surveillance with the broader landscape of biomedical data. In the process, it will increase the vectors by which academics become unwitting or unwilling collaborators with surveillance and data brokers, lending what credibility they have left to a landscape of buggy black boxes of biopolitical control.


